{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 - Question 2\n",
    "\n",
    "## Web References\n",
    "\n",
    "- [k-means document clustering using Apache Mahout command line](https://datasciencetutos.wordpress.com/2016/08/04/k-means-document-clustering-using-apache-mahout-command-line/)\n",
    "- [https://bickson.blogspot.com/2011/09/understanding-mahout-k-means-clustering.html](https://bickson.blogspot.com/2011/09/understanding-mahout-k-means-clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 items\n",
      "-rw-r--r--   3 jfoul001 users     378987 2021-12-31 12:37 dsm010/british-fiction-corpus/ABronte_Agnes.txt\n",
      "-rw-r--r--   3 jfoul001 users     930593 2021-12-31 12:37 dsm010/british-fiction-corpus/ABronte_Tenant.txt\n",
      "-rw-r--r--   3 jfoul001 users     894304 2021-12-31 12:37 dsm010/british-fiction-corpus/Austen_Emma.txt\n",
      "-rw-r--r--   3 jfoul001 users     683758 2021-12-31 12:37 dsm010/british-fiction-corpus/Austen_Pride.txt\n",
      "-rw-r--r--   3 jfoul001 users     678691 2021-12-31 12:37 dsm010/british-fiction-corpus/Austen_Sense.txt\n",
      "-rw-r--r--   3 jfoul001 users    1026320 2021-12-31 12:37 dsm010/british-fiction-corpus/CBronte_Jane.txt\n",
      "-rw-r--r--   3 jfoul001 users     506144 2021-12-31 12:37 dsm010/british-fiction-corpus/CBronte_Professor.txt\n",
      "-rw-r--r--   3 jfoul001 users    1104704 2021-12-31 12:37 dsm010/british-fiction-corpus/CBronte_Villette.txt\n",
      "-rw-r--r--   3 jfoul001 users    1964819 2021-12-31 12:37 dsm010/british-fiction-corpus/Dickens_Bleak.txt\n",
      "-rw-r--r--   3 jfoul001 users    1960784 2021-12-31 12:37 dsm010/british-fiction-corpus/Dickens_David.txt\n",
      "-rw-r--r--   3 jfoul001 users     572724 2021-12-31 12:37 dsm010/british-fiction-corpus/Dickens_Hard.txt\n",
      "-rw-r--r--   3 jfoul001 users     655011 2021-12-31 12:37 dsm010/british-fiction-corpus/EBronte_Wuthering.txt\n",
      "-rw-r--r--   3 jfoul001 users    1169423 2021-12-31 12:37 dsm010/british-fiction-corpus/Eliot_Adam.txt\n",
      "-rw-r--r--   3 jfoul001 users    1803371 2021-12-31 12:37 dsm010/british-fiction-corpus/Eliot_Middlemarch.txt\n",
      "-rw-r--r--   3 jfoul001 users    1158418 2021-12-31 12:37 dsm010/british-fiction-corpus/Eliot_Mill.txt\n",
      "-rw-r--r--   3 jfoul001 users     772698 2021-12-31 12:37 dsm010/british-fiction-corpus/Fielding_Joseph.txt\n",
      "-rw-r--r--   3 jfoul001 users    1961223 2021-12-31 12:37 dsm010/british-fiction-corpus/Fielding_Tom.txt\n",
      "-rw-r--r--   3 jfoul001 users    5263739 2021-12-31 12:37 dsm010/british-fiction-corpus/Richardson_Clarissa.txt\n",
      "-rw-r--r--   3 jfoul001 users    2333073 2021-12-31 12:37 dsm010/british-fiction-corpus/Richardson_Pamela.txt\n",
      "-rw-r--r--   3 jfoul001 users     217007 2021-12-31 12:37 dsm010/british-fiction-corpus/Sterne_Sentimental.txt\n",
      "-rw-r--r--   3 jfoul001 users    1042340 2021-12-31 12:37 dsm010/british-fiction-corpus/Sterne_Tristram.txt\n",
      "-rw-r--r--   3 jfoul001 users     702691 2021-12-31 12:37 dsm010/british-fiction-corpus/Thackeray_Barry.txt\n",
      "-rw-r--r--   3 jfoul001 users    1976584 2021-12-31 12:37 dsm010/british-fiction-corpus/Thackeray_Pendennis.txt\n",
      "-rw-r--r--   3 jfoul001 users    1731407 2021-12-31 12:37 dsm010/british-fiction-corpus/Thackeray_Vanity.txt\n",
      "-rw-r--r--   3 jfoul001 users    1096762 2021-12-31 12:37 dsm010/british-fiction-corpus/Trollope_Barchester.txt\n",
      "-rw-r--r--   3 jfoul001 users    1424627 2021-12-31 12:37 dsm010/british-fiction-corpus/Trollope_Phineas.txt\n",
      "-rw-r--r--   3 jfoul001 users    1531923 2021-12-31 12:37 dsm010/british-fiction-corpus/Trollope_Prime.txt\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:08 dsm010/british-fiction-corpus/british-fiction-corpus\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# change to the coursework directory\n",
    "cd ~/code/dsm010-2021-oct/coursework_01/\n",
    "\n",
    "# copy the input documents\n",
    " hadoop fs -copyFromLocal data/raw/western_classics/british-fiction-corpus dsm010/british-fiction-corpus\n",
    "\n",
    "# verify the file uploads\n",
    "hadoop fs -ls dsm010/british-fiction-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset to SequenceFiles\n",
    "\n",
    "SequenceFiles are flat files consisting of binary key/value pairs. Each document is represented as a key-value pair. there the key is the document id and value is its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/31 14:53:41 INFO AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[5], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[dsm010/british-fiction-corpus], --keyPrefix=[], --method=[mapreduce], --output=[dsm010/british-fiction-corpus-seqfiles], --startPhase=[0], --tempDir=[temp]}\n",
      "21/12/31 14:53:41 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n",
      "21/12/31 14:53:41 INFO deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n",
      "21/12/31 14:53:41 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "21/12/31 14:53:42 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 14:53:42 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226651\n",
      "21/12/31 14:53:42 INFO FileInputFormat: Total input files to process : 27\n",
      "21/12/31 14:53:42 INFO JobSubmitter: number of splits:7\n",
      "21/12/31 14:53:42 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226651\n",
      "21/12/31 14:53:42 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 14:53:42 INFO Configuration: resource-types.xml not found\n",
      "21/12/31 14:53:42 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/12/31 14:53:43 INFO YarnClientImpl: Submitted application application_1626049283275_226651\n",
      "21/12/31 14:53:43 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226651/\n",
      "21/12/31 14:53:43 INFO Job: Running job: job_1626049283275_226651\n",
      "21/12/31 14:53:49 INFO Job: Job job_1626049283275_226651 running in uber mode : false\n",
      "21/12/31 14:53:49 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 14:53:55 INFO Job:  map 86% reduce 0%\n",
      "21/12/31 14:53:56 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 14:53:56 INFO Job: Job job_1626049283275_226651 completed successfully\n",
      "21/12/31 14:53:56 INFO Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=1858745\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=35545415\n",
      "\t\tHDFS: Number of bytes written=13527708\n",
      "\t\tHDFS: Number of read operations=154\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=7\n",
      "\t\tOther local map tasks=2\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=127590\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=25518\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=25518\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=130652160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=28\n",
      "\t\tMap output records=28\n",
      "\t\tInput split bytes=3290\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=175\n",
      "\t\tCPU time spent (ms)=10400\n",
      "\t\tPhysical memory (bytes) snapshot=2042454016\n",
      "\t\tVirtual memory (bytes) snapshot=44687413248\n",
      "\t\tTotal committed heap usage (bytes)=7369392128\n",
      "\t\tPeak Map Physical memory (bytes)=301842432\n",
      "\t\tPeak Map Virtual memory (bytes)=6389805056\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13527708\n",
      "21/12/31 14:53:56 INFO MahoutDriver: Program took 14772 ms (Minutes: 0.2462)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout seqdirectory -i dsm010/british-fiction-corpus -o dsm010/british-fiction-corpus-seqfiles -c UTF-8 -chunk 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "-rw-r--r--   3 jfoul001 users          0 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/_SUCCESS\n",
      "-rw-r--r--   3 jfoul001 users    2503651 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00000\n",
      "-rw-r--r--   3 jfoul001 users    2277756 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00001\n",
      "-rw-r--r--   3 jfoul001 users    2149725 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00002\n",
      "-rw-r--r--   3 jfoul001 users    2043774 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00003\n",
      "-rw-r--r--   3 jfoul001 users    2026798 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00004\n",
      "-rw-r--r--   3 jfoul001 users    1966802 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00005\n",
      "-rw-r--r--   3 jfoul001 users     559202 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00006\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-seqfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sequenceFiles to sparse vector files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/31 15:13:43 INFO SparseVectorsFromSequenceFiles: Maximum n-gram size is: 1\n",
      "21/12/31 15:13:43 INFO SparseVectorsFromSequenceFiles: Minimum LLR value: 1.0\n",
      "21/12/31 15:13:43 INFO SparseVectorsFromSequenceFiles: Number of reduce tasks: 1\n",
      "21/12/31 15:13:43 INFO SparseVectorsFromSequenceFiles: Tokenizing documents in dsm010/british-fiction-corpus-seqfiles\n",
      "21/12/31 15:13:44 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:13:44 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226808\n",
      "21/12/31 15:13:44 INFO FileInputFormat: Total input files to process : 7\n",
      "21/12/31 15:13:44 INFO JobSubmitter: number of splits:7\n",
      "21/12/31 15:13:44 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226808\n",
      "21/12/31 15:13:44 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:13:45 INFO Configuration: resource-types.xml not found\n",
      "21/12/31 15:13:45 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/12/31 15:13:45 INFO YarnClientImpl: Submitted application application_1626049283275_226808\n",
      "21/12/31 15:13:45 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226808/\n",
      "21/12/31 15:13:45 INFO Job: Running job: job_1626049283275_226808\n",
      "21/12/31 15:13:51 INFO Job: Job job_1626049283275_226808 running in uber mode : false\n",
      "21/12/31 15:13:51 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:13:57 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:13:57 INFO Job: Job job_1626049283275_226808 completed successfully\n",
      "21/12/31 15:13:57 INFO Job: Counters: 34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=1855574\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=13528751\n",
      "\t\tHDFS: Number of bytes written=26388375\n",
      "\t\tHDFS: Number of read operations=56\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=7\n",
      "\t\tData-local map tasks=5\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=122685\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=24537\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=24537\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=125629440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=28\n",
      "\t\tMap output records=28\n",
      "\t\tInput split bytes=1043\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=333\n",
      "\t\tCPU time spent (ms)=18420\n",
      "\t\tPhysical memory (bytes) snapshot=2569973760\n",
      "\t\tVirtual memory (bytes) snapshot=44691267584\n",
      "\t\tTotal committed heap usage (bytes)=7369392128\n",
      "\t\tPeak Map Physical memory (bytes)=397729792\n",
      "\t\tPeak Map Virtual memory (bytes)=6394085376\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=13527708\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26388375\n",
      "21/12/31 15:13:57 INFO SparseVectorsFromSequenceFiles: Creating Term Frequency Vectors\n",
      "21/12/31 15:13:57 INFO DictionaryVectorizer: Creating dictionary from dsm010/british-fiction-corpus-vectors/tokenized-documents and saving at dsm010/british-fiction-corpus-vectors/wordcount\n",
      "21/12/31 15:13:57 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:13:57 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226811\n",
      "21/12/31 15:13:57 INFO FileInputFormat: Total input files to process : 7\n",
      "21/12/31 15:13:57 INFO JobSubmitter: number of splits:7\n",
      "21/12/31 15:13:57 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226811\n",
      "21/12/31 15:13:57 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:13:57 INFO YarnClientImpl: Submitted application application_1626049283275_226811\n",
      "21/12/31 15:13:57 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226811/\n",
      "21/12/31 15:13:57 INFO Job: Running job: job_1626049283275_226811\n",
      "21/12/31 15:14:04 INFO Job: Job job_1626049283275_226811 running in uber mode : false\n",
      "21/12/31 15:14:04 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:14:09 INFO Job:  map 14% reduce 0%\n",
      "21/12/31 15:14:10 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:14:14 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:14:14 INFO Job: Job job_1626049283275_226811 completed successfully\n",
      "21/12/31 15:14:14 INFO Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3044595\n",
      "\t\tFILE: Number of bytes written=8214219\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=26389551\n",
      "\t\tHDFS: Number of bytes written=955129\n",
      "\t\tHDFS: Number of read operations=33\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=7\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=121255\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9455\n",
      "\t\tTotal time spent by all map tasks (ms)=24251\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1891\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=24251\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1891\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=124165120\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9681920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=28\n",
      "\t\tMap output records=318679\n",
      "\t\tMap output bytes=5204855\n",
      "\t\tMap output materialized bytes=3044631\n",
      "\t\tInput split bytes=1176\n",
      "\t\tCombine input records=318679\n",
      "\t\tCombine output records=163944\n",
      "\t\tReduce input groups=57492\n",
      "\t\tReduce shuffle bytes=3044631\n",
      "\t\tReduce input records=163944\n",
      "\t\tReduce output records=38756\n",
      "\t\tSpilled Records=327888\n",
      "\t\tShuffled Maps =7\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=7\n",
      "\t\tGC time elapsed (ms)=363\n",
      "\t\tCPU time spent (ms)=23870\n",
      "\t\tPhysical memory (bytes) snapshot=3601731584\n",
      "\t\tVirtual memory (bytes) snapshot=51177414656\n",
      "\t\tTotal committed heap usage (bytes)=8422162432\n",
      "\t\tPeak Map Physical memory (bytes)=497278976\n",
      "\t\tPeak Map Virtual memory (bytes)=6398296064\n",
      "\t\tPeak Reduce Physical memory (bytes)=403537920\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6424883200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=26388375\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=955129\n",
      "21/12/31 15:14:14 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:14:14 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226815\n",
      "21/12/31 15:14:14 INFO FileInputFormat: Total input files to process : 7\n",
      "21/12/31 15:14:14 INFO JobSubmitter: number of splits:7\n",
      "21/12/31 15:14:14 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226815\n",
      "21/12/31 15:14:14 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:14:14 INFO YarnClientImpl: Submitted application application_1626049283275_226815\n",
      "21/12/31 15:14:14 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226815/\n",
      "21/12/31 15:14:14 INFO Job: Running job: job_1626049283275_226815\n",
      "21/12/31 15:14:20 INFO Job: Job job_1626049283275_226815 running in uber mode : false\n",
      "21/12/31 15:14:20 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:14:25 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:14:33 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:14:33 INFO Job: Job job_1626049283275_226815 completed successfully\n",
      "21/12/31 15:14:34 INFO Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=26387254\n",
      "\t\tFILE: Number of bytes written=54913293\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=27189615\n",
      "\t\tHDFS: Number of bytes written=3028543\n",
      "\t\tHDFS: Number of read operations=35\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=7\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=118675\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23605\n",
      "\t\tTotal time spent by all map tasks (ms)=23735\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4721\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=23735\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4721\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=121523200\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=24171520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=28\n",
      "\t\tMap output records=28\n",
      "\t\tMap output bytes=26387108\n",
      "\t\tMap output materialized bytes=26387290\n",
      "\t\tInput split bytes=1176\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=27\n",
      "\t\tReduce shuffle bytes=26387290\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=27\n",
      "\t\tSpilled Records=56\n",
      "\t\tShuffled Maps =7\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=7\n",
      "\t\tGC time elapsed (ms)=667\n",
      "\t\tCPU time spent (ms)=23650\n",
      "\t\tPhysical memory (bytes) snapshot=4200214528\n",
      "\t\tVirtual memory (bytes) snapshot=51062484992\n",
      "\t\tTotal committed heap usage (bytes)=9166651392\n",
      "\t\tPeak Map Physical memory (bytes)=534872064\n",
      "\t\tPeak Map Virtual memory (bytes)=6387892224\n",
      "\t\tPeak Reduce Physical memory (bytes)=834416640\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6398664704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=26388375\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3028543\n",
      "21/12/31 15:14:34 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:14:34 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226819\n",
      "21/12/31 15:14:34 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 15:14:34 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 15:14:34 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226819\n",
      "21/12/31 15:14:34 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:14:34 INFO YarnClientImpl: Submitted application application_1626049283275_226819\n",
      "21/12/31 15:14:34 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226819/\n",
      "21/12/31 15:14:34 INFO Job: Running job: job_1626049283275_226819\n",
      "21/12/31 15:14:40 INFO Job: Job job_1626049283275_226819 running in uber mode : false\n",
      "21/12/31 15:14:40 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:14:45 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:14:51 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:14:51 INFO Job: Job job_1626049283275_226819 completed successfully\n",
      "21/12/31 15:14:51 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3028037\n",
      "\t\tFILE: Number of bytes written=6587977\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3028709\n",
      "\t\tHDFS: Number of bytes written=3028543\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16240\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16505\n",
      "\t\tTotal time spent by all map tasks (ms)=3248\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3301\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3248\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3301\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16629760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16901120\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=27\n",
      "\t\tMap output bytes=3027897\n",
      "\t\tMap output materialized bytes=3028037\n",
      "\t\tInput split bytes=166\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=27\n",
      "\t\tReduce shuffle bytes=3028037\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=27\n",
      "\t\tSpilled Records=54\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=55\n",
      "\t\tCPU time spent (ms)=2800\n",
      "\t\tPhysical memory (bytes) snapshot=703074304\n",
      "\t\tVirtual memory (bytes) snapshot=12772061184\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=393338880\n",
      "\t\tPeak Map Virtual memory (bytes)=6388756480\n",
      "\t\tPeak Reduce Physical memory (bytes)=311214080\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6385270784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3028543\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3028543\n",
      "21/12/31 15:14:51 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/partial-vectors-0\n",
      "21/12/31 15:14:51 INFO SparseVectorsFromSequenceFiles: Calculating IDF\n",
      "21/12/31 15:14:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:14:51 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226823\n",
      "21/12/31 15:14:51 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 15:14:51 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 15:14:52 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226823\n",
      "21/12/31 15:14:52 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:14:52 INFO YarnClientImpl: Submitted application application_1626049283275_226823\n",
      "21/12/31 15:14:52 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226823/\n",
      "21/12/31 15:14:52 INFO Job: Running job: job_1626049283275_226823\n",
      "21/12/31 15:14:58 INFO Job: Job job_1626049283275_226823 running in uber mode : false\n",
      "21/12/31 15:14:58 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:15:03 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:15:09 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:15:09 INFO Job: Job job_1626049283275_226823 completed successfully\n",
      "21/12/31 15:15:09 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=542604\n",
      "\t\tFILE: Number of bytes written=1616211\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3028710\n",
      "\t\tHDFS: Number of bytes written=775373\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17705\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16830\n",
      "\t\tTotal time spent by all map tasks (ms)=3541\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3366\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3541\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3366\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18129920\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17233920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=286077\n",
      "\t\tMap output bytes=3432924\n",
      "\t\tMap output materialized bytes=542604\n",
      "\t\tInput split bytes=167\n",
      "\t\tCombine input records=286077\n",
      "\t\tCombine output records=38757\n",
      "\t\tReduce input groups=38757\n",
      "\t\tReduce shuffle bytes=542604\n",
      "\t\tReduce input records=38757\n",
      "\t\tReduce output records=38757\n",
      "\t\tSpilled Records=77514\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=34\n",
      "\t\tCPU time spent (ms)=4530\n",
      "\t\tPhysical memory (bytes) snapshot=722636800\n",
      "\t\tVirtual memory (bytes) snapshot=12784807936\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=397295616\n",
      "\t\tPeak Map Virtual memory (bytes)=6386257920\n",
      "\t\tPeak Reduce Physical memory (bytes)=325341184\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6398550016\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3028543\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=775373\n",
      "21/12/31 15:15:09 INFO SparseVectorsFromSequenceFiles: Pruning\n",
      "21/12/31 15:15:09 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n",
      "21/12/31 15:15:09 INFO deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n",
      "21/12/31 15:15:09 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "21/12/31 15:15:09 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:15:09 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226826\n",
      "21/12/31 15:15:09 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 15:15:09 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 15:15:09 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226826\n",
      "21/12/31 15:15:09 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:15:10 INFO YarnClientImpl: Submitted application application_1626049283275_226826\n",
      "21/12/31 15:15:10 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226826/\n",
      "21/12/31 15:15:10 INFO Job: Running job: job_1626049283275_226826\n",
      "21/12/31 15:15:16 INFO Job: Job job_1626049283275_226826 running in uber mode : false\n",
      "21/12/31 15:15:16 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:15:22 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:15:28 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:15:28 INFO Job: Job job_1626049283275_226826 completed successfully\n",
      "21/12/31 15:15:28 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1799950\n",
      "\t\tFILE: Number of bytes written=2582411\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3028710\n",
      "\t\tHDFS: Number of bytes written=2680027\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16870\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18090\n",
      "\t\tTotal time spent by all map tasks (ms)=3374\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3618\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3374\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3618\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17274880\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18524160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=27\n",
      "\t\tMap output bytes=3027897\n",
      "\t\tMap output materialized bytes=1024589\n",
      "\t\tInput split bytes=167\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=27\n",
      "\t\tReduce shuffle bytes=1024589\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=27\n",
      "\t\tSpilled Records=54\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=57\n",
      "\t\tCPU time spent (ms)=4520\n",
      "\t\tPhysical memory (bytes) snapshot=759623680\n",
      "\t\tVirtual memory (bytes) snapshot=12797382656\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=379166720\n",
      "\t\tPeak Map Virtual memory (bytes)=6387716096\n",
      "\t\tPeak Reduce Physical memory (bytes)=380456960\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6409666560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3028543\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2680027\n",
      "21/12/31 15:15:28 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:15:28 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226830\n",
      "21/12/31 15:15:28 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 15:15:28 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 15:15:28 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226830\n",
      "21/12/31 15:15:28 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:15:28 INFO YarnClientImpl: Submitted application application_1626049283275_226830\n",
      "21/12/31 15:15:28 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226830/\n",
      "21/12/31 15:15:28 INFO Job: Running job: job_1626049283275_226830\n",
      "21/12/31 15:15:34 INFO Job: Job job_1626049283275_226830 running in uber mode : false\n",
      "21/12/31 15:15:34 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:15:39 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:15:45 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:15:45 INFO Job: Job job_1626049283275_226830 completed successfully\n",
      "21/12/31 15:15:45 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2679517\n",
      "\t\tFILE: Number of bytes written=5890067\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2680204\n",
      "\t\tHDFS: Number of bytes written=2680027\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16580\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16090\n",
      "\t\tTotal time spent by all map tasks (ms)=3316\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3218\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3316\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3218\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16977920\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16476160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=27\n",
      "\t\tMap output bytes=2679381\n",
      "\t\tMap output materialized bytes=2679517\n",
      "\t\tInput split bytes=177\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=27\n",
      "\t\tReduce shuffle bytes=2679517\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=27\n",
      "\t\tSpilled Records=54\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=52\n",
      "\t\tCPU time spent (ms)=2650\n",
      "\t\tPhysical memory (bytes) snapshot=686395392\n",
      "\t\tVirtual memory (bytes) snapshot=12764573696\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=385929216\n",
      "\t\tPeak Map Virtual memory (bytes)=6378713088\n",
      "\t\tPeak Reduce Physical memory (bytes)=300466176\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6389825536\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680027\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2680027\n",
      "21/12/31 15:15:45 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/tf-vectors-partial\n",
      "21/12/31 15:15:45 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/tf-vectors-toprune\n",
      "21/12/31 15:15:45 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:15:45 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226834\n",
      "21/12/31 15:15:46 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 15:15:46 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 15:15:46 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226834\n",
      "21/12/31 15:15:46 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:15:46 INFO YarnClientImpl: Submitted application application_1626049283275_226834\n",
      "21/12/31 15:15:46 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226834/\n",
      "21/12/31 15:15:46 INFO Job: Running job: job_1626049283275_226834\n",
      "21/12/31 15:15:52 INFO Job: Job job_1626049283275_226834 running in uber mode : false\n",
      "21/12/31 15:15:52 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:15:57 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:16:03 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:16:03 INFO Job: Job job_1626049283275_226834 completed successfully\n",
      "21/12/31 15:16:03 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2679517\n",
      "\t\tFILE: Number of bytes written=5893387\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3455539\n",
      "\t\tHDFS: Number of bytes written=2680027\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16565\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16980\n",
      "\t\tTotal time spent by all map tasks (ms)=3313\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3396\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3313\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3396\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16962560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17387520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=27\n",
      "\t\tMap output bytes=2679381\n",
      "\t\tMap output materialized bytes=2679517\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=27\n",
      "\t\tReduce shuffle bytes=2679517\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=27\n",
      "\t\tSpilled Records=54\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=51\n",
      "\t\tCPU time spent (ms)=4040\n",
      "\t\tPhysical memory (bytes) snapshot=733880320\n",
      "\t\tVirtual memory (bytes) snapshot=12784246784\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=378384384\n",
      "\t\tPeak Map Virtual memory (bytes)=6377607168\n",
      "\t\tPeak Reduce Physical memory (bytes)=355495936\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6408622080\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680027\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2680027\n",
      "21/12/31 15:16:03 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 15:16:03 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_226837\n",
      "21/12/31 15:16:03 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 15:16:03 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 15:16:03 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_226837\n",
      "21/12/31 15:16:03 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 15:16:04 INFO YarnClientImpl: Submitted application application_1626049283275_226837\n",
      "21/12/31 15:16:04 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_226837/\n",
      "21/12/31 15:16:04 INFO Job: Running job: job_1626049283275_226837\n",
      "21/12/31 15:16:10 INFO Job: Job job_1626049283275_226837 running in uber mode : false\n",
      "21/12/31 15:16:10 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 15:16:15 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 15:16:20 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 15:16:20 INFO Job: Job job_1626049283275_226837 completed successfully\n",
      "21/12/31 15:16:20 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2679517\n",
      "\t\tFILE: Number of bytes written=5890927\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2680193\n",
      "\t\tHDFS: Number of bytes written=2680027\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16155\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8965\n",
      "\t\tTotal time spent by all map tasks (ms)=3231\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1793\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3231\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1793\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16542720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9180160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=27\n",
      "\t\tMap output bytes=2679381\n",
      "\t\tMap output materialized bytes=2679517\n",
      "\t\tInput split bytes=166\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=27\n",
      "\t\tReduce shuffle bytes=2679517\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=27\n",
      "\t\tSpilled Records=54\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=77\n",
      "\t\tCPU time spent (ms)=2610\n",
      "\t\tPhysical memory (bytes) snapshot=685445120\n",
      "\t\tVirtual memory (bytes) snapshot=12782510080\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=388407296\n",
      "\t\tPeak Map Virtual memory (bytes)=6387785728\n",
      "\t\tPeak Reduce Physical memory (bytes)=297037824\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6395117568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680027\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2680027\n",
      "21/12/31 15:16:20 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/partial-vectors-0\n",
      "21/12/31 15:16:20 INFO MahoutDriver: Program took 156704 ms (Minutes: 2.6117333333333335)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout seq2sparse -nv -i dsm010/british-fiction-corpus-seqfiles -o dsm010/british-fiction-corpus-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/df-count\n",
      "-rw-r--r--   3 jfoul001 users     800064 2021-12-31 15:14 dsm010/british-fiction-corpus-vectors/dictionary.file-0\n",
      "-rw-r--r--   3 jfoul001 users     775353 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/frequency.file-0\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/tf-vectors\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:16 dsm010/british-fiction-corpus-vectors/tfidf-vectors\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:13 dsm010/british-fiction-corpus-vectors/tokenized-documents\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:14 dsm010/british-fiction-corpus-vectors/wordcount\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 jfoul001 users          0 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/tf-vectors/_SUCCESS\n",
      "-rw-r--r--   3 jfoul001 users    2680027 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/tf-vectors/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-vectors/tf-vectors"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "429f3e8d45833d845e6e031dbf3e229703adf0bd2129019c99d8da7ba46dd29e"
  },
  "kernelspec": {
   "display_name": "Bash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
