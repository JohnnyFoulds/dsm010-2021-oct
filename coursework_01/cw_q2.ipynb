{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 - Question 2\n",
    "\n",
    "## Web References\n",
    "\n",
    "- [k-means document clustering using Apache Mahout command line](https://datasciencetutos.wordpress.com/2016/08/04/k-means-document-clustering-using-apache-mahout-command-line/)\n",
    "- [https://bickson.blogspot.com/2011/09/understanding-mahout-k-means-clustering.html](https://bickson.blogspot.com/2011/09/understanding-mahout-k-means-clustering.html)\n",
    "- [Chapter 7. Introduction to clustering â€“ Mahout in Action](http://devguis.com/chapter-7-introduction-to-clustering-mahout-in-action.html)\n",
    "- [Package org.apache.mahout.common.distance](https://mahout.apache.org/docs/0.13.0/api/docs/mahout-mr/org/apache/mahout/common/distance/package-summary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 items\n",
      "-rw-r--r--   3 jfoul001 users     378987 2021-12-31 12:37 dsm010/british-fiction-corpus/ABronte_Agnes.txt\n",
      "-rw-r--r--   3 jfoul001 users     930593 2021-12-31 12:37 dsm010/british-fiction-corpus/ABronte_Tenant.txt\n",
      "-rw-r--r--   3 jfoul001 users     894304 2021-12-31 12:37 dsm010/british-fiction-corpus/Austen_Emma.txt\n",
      "-rw-r--r--   3 jfoul001 users     683758 2021-12-31 12:37 dsm010/british-fiction-corpus/Austen_Pride.txt\n",
      "-rw-r--r--   3 jfoul001 users     678691 2021-12-31 12:37 dsm010/british-fiction-corpus/Austen_Sense.txt\n",
      "-rw-r--r--   3 jfoul001 users    1026320 2021-12-31 12:37 dsm010/british-fiction-corpus/CBronte_Jane.txt\n",
      "-rw-r--r--   3 jfoul001 users     506144 2021-12-31 12:37 dsm010/british-fiction-corpus/CBronte_Professor.txt\n",
      "-rw-r--r--   3 jfoul001 users    1104704 2021-12-31 12:37 dsm010/british-fiction-corpus/CBronte_Villette.txt\n",
      "-rw-r--r--   3 jfoul001 users    1964819 2021-12-31 12:37 dsm010/british-fiction-corpus/Dickens_Bleak.txt\n",
      "-rw-r--r--   3 jfoul001 users    1960784 2021-12-31 12:37 dsm010/british-fiction-corpus/Dickens_David.txt\n",
      "-rw-r--r--   3 jfoul001 users     572724 2021-12-31 12:37 dsm010/british-fiction-corpus/Dickens_Hard.txt\n",
      "-rw-r--r--   3 jfoul001 users     655011 2021-12-31 12:37 dsm010/british-fiction-corpus/EBronte_Wuthering.txt\n",
      "-rw-r--r--   3 jfoul001 users    1169423 2021-12-31 12:37 dsm010/british-fiction-corpus/Eliot_Adam.txt\n",
      "-rw-r--r--   3 jfoul001 users    1803371 2021-12-31 12:37 dsm010/british-fiction-corpus/Eliot_Middlemarch.txt\n",
      "-rw-r--r--   3 jfoul001 users    1158418 2021-12-31 12:37 dsm010/british-fiction-corpus/Eliot_Mill.txt\n",
      "-rw-r--r--   3 jfoul001 users     772698 2021-12-31 12:37 dsm010/british-fiction-corpus/Fielding_Joseph.txt\n",
      "-rw-r--r--   3 jfoul001 users    1961223 2021-12-31 12:37 dsm010/british-fiction-corpus/Fielding_Tom.txt\n",
      "-rw-r--r--   3 jfoul001 users    5263739 2021-12-31 12:37 dsm010/british-fiction-corpus/Richardson_Clarissa.txt\n",
      "-rw-r--r--   3 jfoul001 users    2333073 2021-12-31 12:37 dsm010/british-fiction-corpus/Richardson_Pamela.txt\n",
      "-rw-r--r--   3 jfoul001 users     217007 2021-12-31 12:37 dsm010/british-fiction-corpus/Sterne_Sentimental.txt\n",
      "-rw-r--r--   3 jfoul001 users    1042340 2021-12-31 12:37 dsm010/british-fiction-corpus/Sterne_Tristram.txt\n",
      "-rw-r--r--   3 jfoul001 users     702691 2021-12-31 12:37 dsm010/british-fiction-corpus/Thackeray_Barry.txt\n",
      "-rw-r--r--   3 jfoul001 users    1976584 2021-12-31 12:37 dsm010/british-fiction-corpus/Thackeray_Pendennis.txt\n",
      "-rw-r--r--   3 jfoul001 users    1731407 2021-12-31 12:37 dsm010/british-fiction-corpus/Thackeray_Vanity.txt\n",
      "-rw-r--r--   3 jfoul001 users    1096762 2021-12-31 12:37 dsm010/british-fiction-corpus/Trollope_Barchester.txt\n",
      "-rw-r--r--   3 jfoul001 users    1424627 2021-12-31 12:37 dsm010/british-fiction-corpus/Trollope_Phineas.txt\n",
      "-rw-r--r--   3 jfoul001 users    1531923 2021-12-31 12:37 dsm010/british-fiction-corpus/Trollope_Prime.txt\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:08 dsm010/british-fiction-corpus/british-fiction-corpus\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# change to the coursework directory\n",
    "cd ~/code/dsm010-2021-oct/coursework_01/\n",
    "\n",
    "# copy the input documents\n",
    " hadoop fs -copyFromLocal data/raw/western_classics/british-fiction-corpus dsm010/british-fiction-corpus\n",
    "\n",
    "# verify the file uploads\n",
    "hadoop fs -ls dsm010/british-fiction-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset to SequenceFiles\n",
    "\n",
    "SequenceFiles are flat files consisting of binary key/value pairs. Each document is represented as a key-value pair. there the key is the document id and value is its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/01 12:31:14 INFO AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[5], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[dsm010/british-fiction-corpus], --keyPrefix=[], --method=[mapreduce], --output=[dsm010/british-fiction-corpus-seqfiles], --startPhase=[0], --tempDir=[temp]}\n",
      "22/01/01 12:31:14 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n",
      "22/01/01 12:31:14 INFO deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n",
      "22/01/01 12:31:14 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "22/01/01 12:31:15 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:31:15 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238840\n",
      "22/01/01 12:31:16 INFO FileInputFormat: Total input files to process : 55\n",
      "22/01/01 12:31:16 INFO JobSubmitter: number of splits:13\n",
      "22/01/01 12:31:16 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238840\n",
      "22/01/01 12:31:16 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:31:16 INFO Configuration: resource-types.xml not found\n",
      "22/01/01 12:31:16 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/01/01 12:31:16 INFO YarnClientImpl: Submitted application application_1626049283275_238840\n",
      "22/01/01 12:31:16 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238840/\n",
      "22/01/01 12:31:16 INFO Job: Running job: job_1626049283275_238840\n",
      "22/01/01 12:31:22 INFO Job: Job job_1626049283275_238840 running in uber mode : false\n",
      "22/01/01 12:31:22 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:31:28 INFO Job:  map 92% reduce 0%\n",
      "22/01/01 12:31:29 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:31:29 INFO Job: Job job_1626049283275_238840 completed successfully\n",
      "22/01/01 12:31:29 INFO Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=3453089\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=71091399\n",
      "\t\tHDFS: Number of bytes written=27055959\n",
      "\t\tHDFS: Number of read operations=302\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=26\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tOther local map tasks=2\n",
      "\t\tData-local map tasks=7\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=233455\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=46691\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=46691\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=239057920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=56\n",
      "\t\tMap output records=56\n",
      "\t\tInput split bytes=7149\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=301\n",
      "\t\tCPU time spent (ms)=18940\n",
      "\t\tPhysical memory (bytes) snapshot=3816620032\n",
      "\t\tVirtual memory (bytes) snapshot=82980757504\n",
      "\t\tTotal committed heap usage (bytes)=13686013952\n",
      "\t\tPeak Map Physical memory (bytes)=302231552\n",
      "\t\tPeak Map Virtual memory (bytes)=6394089472\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27055959\n",
      "22/01/01 12:31:29 INFO MahoutDriver: Program took 15156 ms (Minutes: 0.2526)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout seqdirectory -i dsm010/british-fiction-corpus -o dsm010/british-fiction-corpus-seqfiles -c UTF-8 -chunk 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "-rw-r--r--   3 jfoul001 users          0 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/_SUCCESS\n",
      "-rw-r--r--   3 jfoul001 users    2503651 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00000\n",
      "-rw-r--r--   3 jfoul001 users    2277756 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00001\n",
      "-rw-r--r--   3 jfoul001 users    2149725 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00002\n",
      "-rw-r--r--   3 jfoul001 users    2043774 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00003\n",
      "-rw-r--r--   3 jfoul001 users    2026798 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00004\n",
      "-rw-r--r--   3 jfoul001 users    1966802 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00005\n",
      "-rw-r--r--   3 jfoul001 users     559202 2021-12-31 14:53 dsm010/british-fiction-corpus-seqfiles/part-m-00006\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-seqfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sequenceFiles to sparse vector files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/01 12:31:46 INFO SparseVectorsFromSequenceFiles: Maximum n-gram size is: 1\n",
      "22/01/01 12:31:46 INFO SparseVectorsFromSequenceFiles: Minimum LLR value: 1.0\n",
      "22/01/01 12:31:46 INFO SparseVectorsFromSequenceFiles: Number of reduce tasks: 1\n",
      "22/01/01 12:31:46 INFO SparseVectorsFromSequenceFiles: Tokenizing documents in dsm010/british-fiction-corpus-seqfiles\n",
      "22/01/01 12:31:47 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:31:47 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238843\n",
      "22/01/01 12:31:48 INFO FileInputFormat: Total input files to process : 13\n",
      "22/01/01 12:31:48 INFO JobSubmitter: number of splits:13\n",
      "22/01/01 12:31:48 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238843\n",
      "22/01/01 12:31:48 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:31:48 INFO Configuration: resource-types.xml not found\n",
      "22/01/01 12:31:48 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/01/01 12:31:48 INFO YarnClientImpl: Submitted application application_1626049283275_238843\n",
      "22/01/01 12:31:48 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238843/\n",
      "22/01/01 12:31:48 INFO Job: Running job: job_1626049283275_238843\n",
      "22/01/01 12:31:54 INFO Job: Job job_1626049283275_238843 running in uber mode : false\n",
      "22/01/01 12:31:54 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:32:01 INFO Job:  map 85% reduce 0%\n",
      "22/01/01 12:32:02 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:32:02 INFO Job: Job job_1626049283275_238843 completed successfully\n",
      "22/01/01 12:32:02 INFO Job: Counters: 34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=3446095\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=27057896\n",
      "\t\tHDFS: Number of bytes written=52777325\n",
      "\t\tHDFS: Number of read operations=104\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=26\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tData-local map tasks=8\n",
      "\t\tRack-local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=237690\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=47538\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47538\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=243394560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=56\n",
      "\t\tMap output records=56\n",
      "\t\tInput split bytes=1937\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=703\n",
      "\t\tCPU time spent (ms)=36370\n",
      "\t\tPhysical memory (bytes) snapshot=4907925504\n",
      "\t\tVirtual memory (bytes) snapshot=82989670400\n",
      "\t\tTotal committed heap usage (bytes)=13686013952\n",
      "\t\tPeak Map Physical memory (bytes)=413331456\n",
      "\t\tPeak Map Virtual memory (bytes)=6395723776\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=27055959\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52777325\n",
      "22/01/01 12:32:02 INFO SparseVectorsFromSequenceFiles: Creating Term Frequency Vectors\n",
      "22/01/01 12:32:02 INFO DictionaryVectorizer: Creating dictionary from dsm010/british-fiction-corpus-vectors/tokenized-documents and saving at dsm010/british-fiction-corpus-vectors/wordcount\n",
      "22/01/01 12:32:02 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:32:02 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238844\n",
      "22/01/01 12:32:02 INFO FileInputFormat: Total input files to process : 13\n",
      "22/01/01 12:32:02 INFO JobSubmitter: number of splits:13\n",
      "22/01/01 12:32:02 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238844\n",
      "22/01/01 12:32:02 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:32:02 INFO YarnClientImpl: Submitted application application_1626049283275_238844\n",
      "22/01/01 12:32:02 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238844/\n",
      "22/01/01 12:32:02 INFO Job: Running job: job_1626049283275_238844\n",
      "22/01/01 12:32:08 INFO Job: Job job_1626049283275_238844 running in uber mode : false\n",
      "22/01/01 12:32:08 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:32:14 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:32:18 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:32:19 INFO Job: Job job_1626049283275_238844 completed successfully\n",
      "22/01/01 12:32:19 INFO Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6023646\n",
      "\t\tFILE: Number of bytes written=15766174\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=52779509\n",
      "\t\tHDFS: Number of bytes written=1424912\n",
      "\t\tHDFS: Number of read operations=57\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=12\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=227655\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10320\n",
      "\t\tTotal time spent by all map tasks (ms)=45531\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2064\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=45531\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2064\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=233118720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10567680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=56\n",
      "\t\tMap output records=637358\n",
      "\t\tMap output bytes=10409710\n",
      "\t\tMap output materialized bytes=6023718\n",
      "\t\tInput split bytes=2184\n",
      "\t\tCombine input records=637358\n",
      "\t\tCombine output records=324179\n",
      "\t\tReduce input groups=57492\n",
      "\t\tReduce shuffle bytes=6023718\n",
      "\t\tReduce input records=324179\n",
      "\t\tReduce output records=57492\n",
      "\t\tSpilled Records=648358\n",
      "\t\tShuffled Maps =13\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=13\n",
      "\t\tGC time elapsed (ms)=659\n",
      "\t\tCPU time spent (ms)=42230\n",
      "\t\tPhysical memory (bytes) snapshot=6404624384\n",
      "\t\tVirtual memory (bytes) snapshot=89477160960\n",
      "\t\tTotal committed heap usage (bytes)=14738784256\n",
      "\t\tPeak Map Physical memory (bytes)=505143296\n",
      "\t\tPeak Map Virtual memory (bytes)=6402297856\n",
      "\t\tPeak Reduce Physical memory (bytes)=419028992\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6421839872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52777325\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1424912\n",
      "22/01/01 12:32:19 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:32:19 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238846\n",
      "22/01/01 12:32:20 INFO FileInputFormat: Total input files to process : 13\n",
      "22/01/01 12:32:20 INFO JobSubmitter: number of splits:13\n",
      "22/01/01 12:32:20 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238846\n",
      "22/01/01 12:32:20 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:32:20 INFO YarnClientImpl: Submitted application application_1626049283275_238846\n",
      "22/01/01 12:32:20 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238846/\n",
      "22/01/01 12:32:20 INFO Job: Running job: job_1626049283275_238846\n",
      "22/01/01 12:32:25 INFO Job: Job job_1626049283275_238846 running in uber mode : false\n",
      "22/01/01 12:32:25 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:32:31 INFO Job:  map 92% reduce 0%\n",
      "22/01/01 12:32:32 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:32:38 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:32:38 INFO Job: Job job_1626049283275_238846 completed successfully\n",
      "22/01/01 12:32:38 INFO Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=52775146\n",
      "\t\tFILE: Number of bytes written=109293264\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=53974412\n",
      "\t\tHDFS: Number of bytes written=6545088\n",
      "\t\tHDFS: Number of read operations=59\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=11\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=220515\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23325\n",
      "\t\tTotal time spent by all map tasks (ms)=44103\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4665\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=44103\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4665\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=225807360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=23884800\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=56\n",
      "\t\tMap output records=56\n",
      "\t\tMap output bytes=52774860\n",
      "\t\tMap output materialized bytes=52775218\n",
      "\t\tInput split bytes=2184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=54\n",
      "\t\tReduce shuffle bytes=52775218\n",
      "\t\tReduce input records=56\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=112\n",
      "\t\tShuffled Maps =13\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=13\n",
      "\t\tGC time elapsed (ms)=1364\n",
      "\t\tCPU time spent (ms)=44120\n",
      "\t\tPhysical memory (bytes) snapshot=7579017216\n",
      "\t\tVirtual memory (bytes) snapshot=89365254144\n",
      "\t\tTotal committed heap usage (bytes)=16345202688\n",
      "\t\tPeak Map Physical memory (bytes)=570593280\n",
      "\t\tPeak Map Virtual memory (bytes)=6387351552\n",
      "\t\tPeak Reduce Physical memory (bytes)=1423364096\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6393430016\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52777325\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6545088\n",
      "22/01/01 12:32:38 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:32:38 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238848\n",
      "22/01/01 12:32:39 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:32:39 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:32:39 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238848\n",
      "22/01/01 12:32:39 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:32:39 INFO YarnClientImpl: Submitted application application_1626049283275_238848\n",
      "22/01/01 12:32:39 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238848/\n",
      "22/01/01 12:32:39 INFO Job: Running job: job_1626049283275_238848\n",
      "22/01/01 12:32:45 INFO Job: Job job_1626049283275_238848 running in uber mode : false\n",
      "22/01/01 12:32:45 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:32:50 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:32:55 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:32:55 INFO Job: Job job_1626049283275_238848 completed successfully\n",
      "22/01/01 12:32:55 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6544020\n",
      "\t\tFILE: Number of bytes written=13619943\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6545254\n",
      "\t\tHDFS: Number of bytes written=6545088\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16295\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9715\n",
      "\t\tTotal time spent by all map tasks (ms)=3259\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1943\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3259\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1943\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16686080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9948160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=54\n",
      "\t\tMap output bytes=6543746\n",
      "\t\tMap output materialized bytes=6544020\n",
      "\t\tInput split bytes=166\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=54\n",
      "\t\tReduce shuffle bytes=6544020\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=108\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=34\n",
      "\t\tCPU time spent (ms)=3230\n",
      "\t\tPhysical memory (bytes) snapshot=698724352\n",
      "\t\tVirtual memory (bytes) snapshot=12767883264\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=399679488\n",
      "\t\tPeak Map Virtual memory (bytes)=6380744704\n",
      "\t\tPeak Reduce Physical memory (bytes)=302411776\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6391099392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6545088\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6545088\n",
      "22/01/01 12:32:55 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/partial-vectors-0\n",
      "22/01/01 12:32:55 INFO SparseVectorsFromSequenceFiles: Calculating IDF\n",
      "22/01/01 12:32:55 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:32:55 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238851\n",
      "22/01/01 12:32:55 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:32:55 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:32:55 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238851\n",
      "22/01/01 12:32:55 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:32:56 INFO YarnClientImpl: Submitted application application_1626049283275_238851\n",
      "22/01/01 12:32:56 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238851/\n",
      "22/01/01 12:32:56 INFO Job: Running job: job_1626049283275_238851\n",
      "22/01/01 12:33:02 INFO Job: Job job_1626049283275_238851 running in uber mode : false\n",
      "22/01/01 12:33:02 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:33:08 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:33:11 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:33:12 INFO Job: Job job_1626049283275_238851 completed successfully\n",
      "22/01/01 12:33:12 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=804908\n",
      "\t\tFILE: Number of bytes written=2140819\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6545255\n",
      "\t\tHDFS: Number of bytes written=1150173\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19090\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9360\n",
      "\t\tTotal time spent by all map tasks (ms)=3818\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1872\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3818\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1872\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19548160\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9584640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=609626\n",
      "\t\tMap output bytes=7315512\n",
      "\t\tMap output materialized bytes=804908\n",
      "\t\tInput split bytes=167\n",
      "\t\tCombine input records=609626\n",
      "\t\tCombine output records=57493\n",
      "\t\tReduce input groups=57493\n",
      "\t\tReduce shuffle bytes=804908\n",
      "\t\tReduce input records=57493\n",
      "\t\tReduce output records=57493\n",
      "\t\tSpilled Records=114986\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=51\n",
      "\t\tCPU time spent (ms)=5070\n",
      "\t\tPhysical memory (bytes) snapshot=713121792\n",
      "\t\tVirtual memory (bytes) snapshot=12782960640\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=390053888\n",
      "\t\tPeak Map Virtual memory (bytes)=6382051328\n",
      "\t\tPeak Reduce Physical memory (bytes)=323067904\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6404038656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6545088\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1150173\n",
      "22/01/01 12:33:12 INFO SparseVectorsFromSequenceFiles: Pruning\n",
      "22/01/01 12:33:12 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n",
      "22/01/01 12:33:12 INFO deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n",
      "22/01/01 12:33:12 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "22/01/01 12:33:12 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:33:12 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238853\n",
      "22/01/01 12:33:12 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:33:12 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:33:12 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238853\n",
      "22/01/01 12:33:12 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:33:12 INFO YarnClientImpl: Submitted application application_1626049283275_238853\n",
      "22/01/01 12:33:12 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238853/\n",
      "22/01/01 12:33:12 INFO Job: Running job: job_1626049283275_238853\n",
      "22/01/01 12:33:18 INFO Job: Job job_1626049283275_238853 running in uber mode : false\n",
      "22/01/01 12:33:18 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:33:24 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:33:30 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:33:30 INFO Job: Job job_1626049283275_238853 completed successfully\n",
      "22/01/01 12:33:30 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3353344\n",
      "\t\tFILE: Number of bytes written=4939601\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6545255\n",
      "\t\tHDFS: Number of bytes written=5837188\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18250\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18665\n",
      "\t\tTotal time spent by all map tasks (ms)=3650\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3733\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3650\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3733\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18688000\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=19112960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=54\n",
      "\t\tMap output bytes=6543746\n",
      "\t\tMap output materialized bytes=2203183\n",
      "\t\tInput split bytes=167\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=54\n",
      "\t\tReduce shuffle bytes=2203183\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=108\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=42\n",
      "\t\tCPU time spent (ms)=4730\n",
      "\t\tPhysical memory (bytes) snapshot=769069056\n",
      "\t\tVirtual memory (bytes) snapshot=12779638784\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=394244096\n",
      "\t\tPeak Map Virtual memory (bytes)=6377467904\n",
      "\t\tPeak Reduce Physical memory (bytes)=374824960\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6402170880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6545088\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837188\n",
      "22/01/01 12:33:30 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:33:30 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238855\n",
      "22/01/01 12:33:31 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:33:31 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:33:31 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238855\n",
      "22/01/01 12:33:31 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:33:31 INFO YarnClientImpl: Submitted application application_1626049283275_238855\n",
      "22/01/01 12:33:31 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238855/\n",
      "22/01/01 12:33:31 INFO Job: Running job: job_1626049283275_238855\n",
      "22/01/01 12:33:37 INFO Job: Job job_1626049283275_238855 running in uber mode : false\n",
      "22/01/01 12:33:37 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:33:42 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:33:48 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:33:48 INFO Job: Job job_1626049283275_238855 completed successfully\n",
      "22/01/01 12:33:48 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5836232\n",
      "\t\tFILE: Number of bytes written=12203497\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837365\n",
      "\t\tHDFS: Number of bytes written=5837188\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16645\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16650\n",
      "\t\tTotal time spent by all map tasks (ms)=3329\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3330\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3329\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3330\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17044480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17049600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=54\n",
      "\t\tMap output bytes=5835966\n",
      "\t\tMap output materialized bytes=5836232\n",
      "\t\tInput split bytes=177\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=54\n",
      "\t\tReduce shuffle bytes=5836232\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=108\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=75\n",
      "\t\tCPU time spent (ms)=3060\n",
      "\t\tPhysical memory (bytes) snapshot=726544384\n",
      "\t\tVirtual memory (bytes) snapshot=12775186432\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=411955200\n",
      "\t\tPeak Map Virtual memory (bytes)=6389997568\n",
      "\t\tPeak Reduce Physical memory (bytes)=314589184\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6385188864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837188\n",
      "22/01/01 12:33:48 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/tf-vectors-partial\n",
      "22/01/01 12:33:48 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/tf-vectors-toprune\n",
      "22/01/01 12:33:48 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:33:48 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238857\n",
      "22/01/01 12:33:48 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:33:48 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:33:48 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238857\n",
      "22/01/01 12:33:48 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:33:48 INFO YarnClientImpl: Submitted application application_1626049283275_238857\n",
      "22/01/01 12:33:48 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238857/\n",
      "22/01/01 12:33:48 INFO Job: Running job: job_1626049283275_238857\n",
      "22/01/01 12:33:55 INFO Job: Job job_1626049283275_238857 running in uber mode : false\n",
      "22/01/01 12:33:55 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:34:00 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:34:06 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:34:07 INFO Job: Job job_1626049283275_238857 completed successfully\n",
      "22/01/01 12:34:07 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5836232\n",
      "\t\tFILE: Number of bytes written=12206819\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6987500\n",
      "\t\tHDFS: Number of bytes written=5837188\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16335\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17725\n",
      "\t\tTotal time spent by all map tasks (ms)=3267\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3545\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3267\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3545\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16727040\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18150400\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=54\n",
      "\t\tMap output bytes=5835966\n",
      "\t\tMap output materialized bytes=5836232\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=54\n",
      "\t\tReduce shuffle bytes=5836232\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=108\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=87\n",
      "\t\tCPU time spent (ms)=4100\n",
      "\t\tPhysical memory (bytes) snapshot=737538048\n",
      "\t\tVirtual memory (bytes) snapshot=12792188928\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=389210112\n",
      "\t\tPeak Map Virtual memory (bytes)=6385852416\n",
      "\t\tPeak Reduce Physical memory (bytes)=348327936\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6406336512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837188\n",
      "22/01/01 12:34:07 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:34:07 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238859\n",
      "22/01/01 12:34:07 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:34:07 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:34:07 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238859\n",
      "22/01/01 12:34:07 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:34:07 INFO YarnClientImpl: Submitted application application_1626049283275_238859\n",
      "22/01/01 12:34:07 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238859/\n",
      "22/01/01 12:34:07 INFO Job: Running job: job_1626049283275_238859\n",
      "22/01/01 12:34:13 INFO Job: Job job_1626049283275_238859 running in uber mode : false\n",
      "22/01/01 12:34:13 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:34:18 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:34:24 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:34:24 INFO Job: Job job_1626049283275_238859 completed successfully\n",
      "22/01/01 12:34:24 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5836232\n",
      "\t\tFILE: Number of bytes written=12204357\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837354\n",
      "\t\tHDFS: Number of bytes written=5837188\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16395\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16650\n",
      "\t\tTotal time spent by all map tasks (ms)=3279\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3330\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3279\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3330\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16788480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17049600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=54\n",
      "\t\tMap output bytes=5835966\n",
      "\t\tMap output materialized bytes=5836232\n",
      "\t\tInput split bytes=166\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=54\n",
      "\t\tReduce shuffle bytes=5836232\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=108\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=52\n",
      "\t\tCPU time spent (ms)=2910\n",
      "\t\tPhysical memory (bytes) snapshot=683700224\n",
      "\t\tVirtual memory (bytes) snapshot=12767305728\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=393609216\n",
      "\t\tPeak Map Virtual memory (bytes)=6377394176\n",
      "\t\tPeak Reduce Physical memory (bytes)=290091008\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6390505472\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837188\n",
      "22/01/01 12:34:24 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-vectors/partial-vectors-0\n",
      "22/01/01 12:34:24 INFO MahoutDriver: Program took 158054 ms (Minutes: 2.6342333333333334)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout seq2sparse -nv -i dsm010/british-fiction-corpus-seqfiles -o dsm010/british-fiction-corpus-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/df-count\n",
      "-rw-r--r--   3 jfoul001 users     800064 2021-12-31 15:14 dsm010/british-fiction-corpus-vectors/dictionary.file-0\n",
      "-rw-r--r--   3 jfoul001 users     775353 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/frequency.file-0\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/tf-vectors\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:16 dsm010/british-fiction-corpus-vectors/tfidf-vectors\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:13 dsm010/british-fiction-corpus-vectors/tokenized-documents\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 15:14 dsm010/british-fiction-corpus-vectors/wordcount\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 jfoul001 users          0 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/tf-vectors/_SUCCESS\n",
      "-rw-r--r--   3 jfoul001 users    2680027 2021-12-31 15:15 dsm010/british-fiction-corpus-vectors/tf-vectors/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-vectors/tf-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canopy Clustering\n",
    "\n",
    "To have initial centroids values for the k-means algorithm, run canopy clustering on the TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/31 18:13:01 INFO AbstractJob: Command line arguments: {--distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[dsm010/british-fiction-corpus-vectors/tf-vectors], --method=[mapreduce], --output=[dsm010/british-fiction-corpus-canopy-centroids], --overwrite=null, --startPhase=[0], --t1=[0.5], --t2=[0.3], --tempDir=[temp]}\n",
      "21/12/31 18:13:02 INFO CanopyDriver: Build Clusters Input: dsm010/british-fiction-corpus-vectors/tf-vectors Out: dsm010/british-fiction-corpus-canopy-centroids Measure: org.apache.mahout.common.distance.CosineDistanceMeasure@4c08a3e0 t1: 0.5 t2: 0.3\n",
      "21/12/31 18:13:02 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:13:02 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229361\n",
      "21/12/31 18:13:03 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:13:03 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:13:03 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229361\n",
      "21/12/31 18:13:03 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:13:03 INFO Configuration: resource-types.xml not found\n",
      "21/12/31 18:13:03 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/12/31 18:13:03 INFO YarnClientImpl: Submitted application application_1626049283275_229361\n",
      "21/12/31 18:13:03 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229361/\n",
      "21/12/31 18:13:03 INFO Job: Running job: job_1626049283275_229361\n",
      "21/12/31 18:13:09 INFO Job: Job job_1626049283275_229361 running in uber mode : false\n",
      "21/12/31 18:13:09 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:13:14 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:13:20 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:13:21 INFO Job: Job job_1626049283275_229361 completed successfully\n",
      "21/12/31 18:13:21 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2749672\n",
      "\t\tFILE: Number of bytes written=6032973\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2680186\n",
      "\t\tHDFS: Number of bytes written=2763394\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17610\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17610\n",
      "\t\tTotal time spent by all map tasks (ms)=3522\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3522\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3522\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3522\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18032640\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18032640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=27\n",
      "\t\tMap output bytes=2749536\n",
      "\t\tMap output materialized bytes=2749672\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=2749672\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=26\n",
      "\t\tSpilled Records=54\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=98\n",
      "\t\tCPU time spent (ms)=4320\n",
      "\t\tPhysical memory (bytes) snapshot=731373568\n",
      "\t\tVirtual memory (bytes) snapshot=12773416960\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=406446080\n",
      "\t\tPeak Map Virtual memory (bytes)=6376550400\n",
      "\t\tPeak Reduce Physical memory (bytes)=324927488\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6396866560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680027\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2763394\n",
      "21/12/31 18:13:21 INFO MahoutDriver: Program took 20034 ms (Minutes: 0.3339)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout canopy -i dsm010/british-fiction-corpus-vectors/tf-vectors -ow -o dsm010/british-fiction-corpus-canopy-centroids -dm org.apache.mahout.common.distance.CosineDistanceMeasure -t1 0.5 -t2 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 18:13 dsm010/british-fiction-corpus-canopy-centroids/clusters-0-final\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-canopy-centroids/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/31 18:23:32 INFO AbstractJob: Command line arguments: {--clustering=null, --clusters=[dsm010/british-fiction-corpus-canopy-centroids], --convergenceDelta=[0.1], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[dsm010/british-fiction-corpus-vectors/tf-vectors], --maxIter=[20], --method=[mapreduce], --numClusters=[10], --output=[hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans-clusters], --overwrite=null, --startPhase=[0], --tempDir=[temp]}\n",
      "21/12/31 18:23:32 INFO HadoopUtil: Deleting hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans-clusters\n",
      "21/12/31 18:23:32 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-canopy-centroids\n",
      "21/12/31 18:23:33 INFO ZlibFactory: Successfully loaded & initialized native-zlib library\n",
      "21/12/31 18:23:33 INFO CodecPool: Got brand-new compressor [.deflate]\n",
      "21/12/31 18:23:33 INFO RandomSeedGenerator: Wrote 10 Klusters to dsm010/british-fiction-corpus-canopy-centroids/part-randomSeed\n",
      "21/12/31 18:23:33 INFO KMeansDriver: Input: dsm010/british-fiction-corpus-vectors/tf-vectors Clusters In: dsm010/british-fiction-corpus-canopy-centroids/part-randomSeed Out: hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans-clusters\n",
      "21/12/31 18:23:33 INFO KMeansDriver: convergence: 0.1 max Iterations: 20\n",
      "21/12/31 18:23:33 INFO CodecPool: Got brand-new decompressor [.deflate]\n",
      "21/12/31 18:23:33 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:23:33 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229517\n",
      "21/12/31 18:23:34 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:23:34 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:23:34 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229517\n",
      "21/12/31 18:23:34 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:23:34 INFO Configuration: resource-types.xml not found\n",
      "21/12/31 18:23:34 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/12/31 18:23:34 INFO YarnClientImpl: Submitted application application_1626049283275_229517\n",
      "21/12/31 18:23:34 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229517/\n",
      "21/12/31 18:23:34 INFO Job: Running job: job_1626049283275_229517\n",
      "21/12/31 18:23:40 INFO Job: Job job_1626049283275_229517 running in uber mode : false\n",
      "21/12/31 18:23:40 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:23:46 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:23:51 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:23:52 INFO Job: Job job_1626049283275_229517 completed successfully\n",
      "21/12/31 18:23:52 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4293685\n",
      "\t\tFILE: Number of bytes written=9119439\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8988604\n",
      "\t\tHDFS: Number of bytes written=2764438\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18180\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17995\n",
      "\t\tTotal time spent by all map tasks (ms)=3636\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3599\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3636\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3599\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18616320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18426880\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=4293629\n",
      "\t\tMap output materialized bytes=4293685\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=4293685\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=56\n",
      "\t\tCPU time spent (ms)=4930\n",
      "\t\tPhysical memory (bytes) snapshot=720666624\n",
      "\t\tVirtual memory (bytes) snapshot=12775501824\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=404615168\n",
      "\t\tPeak Map Virtual memory (bytes)=6382415872\n",
      "\t\tPeak Reduce Physical memory (bytes)=316051456\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6393085952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680027\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2764438\n",
      "21/12/31 18:23:52 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:23:52 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229522\n",
      "21/12/31 18:23:53 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:23:53 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:23:53 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229522\n",
      "21/12/31 18:23:53 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:23:53 INFO YarnClientImpl: Submitted application application_1626049283275_229522\n",
      "21/12/31 18:23:53 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229522/\n",
      "21/12/31 18:23:53 INFO Job: Running job: job_1626049283275_229522\n",
      "21/12/31 18:23:59 INFO Job: Job job_1626049283275_229522 running in uber mode : false\n",
      "21/12/31 18:23:59 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:24:05 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:24:10 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:24:11 INFO Job: Job job_1626049283275_229522 completed successfully\n",
      "21/12/31 18:24:11 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6005203\n",
      "\t\tFILE: Number of bytes written=12542475\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8209450\n",
      "\t\tHDFS: Number of bytes written=2764438\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18645\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17800\n",
      "\t\tTotal time spent by all map tasks (ms)=3729\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3560\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3729\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3560\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19092480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18227200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=6005147\n",
      "\t\tMap output materialized bytes=6005203\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=6005203\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=44\n",
      "\t\tCPU time spent (ms)=4580\n",
      "\t\tPhysical memory (bytes) snapshot=742756352\n",
      "\t\tVirtual memory (bytes) snapshot=12781371392\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=424632320\n",
      "\t\tPeak Map Virtual memory (bytes)=6392561664\n",
      "\t\tPeak Reduce Physical memory (bytes)=318124032\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6388809728\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680027\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2764438\n",
      "21/12/31 18:24:11 INFO KMeansDriver: Clustering data\n",
      "21/12/31 18:24:11 INFO KMeansDriver: Running Clustering\n",
      "21/12/31 18:24:11 INFO KMeansDriver: Input: dsm010/british-fiction-corpus-vectors/tf-vectors Clusters In: hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans-clusters Out: hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans-clusters\n",
      "21/12/31 18:24:11 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:24:11 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229527\n",
      "21/12/31 18:24:11 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:24:11 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:24:11 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229527\n",
      "21/12/31 18:24:11 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:24:12 INFO YarnClientImpl: Submitted application application_1626049283275_229527\n",
      "21/12/31 18:24:12 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229527/\n",
      "21/12/31 18:24:12 INFO Job: Running job: job_1626049283275_229527\n",
      "21/12/31 18:24:18 INFO Job: Job job_1626049283275_229527 running in uber mode : false\n",
      "21/12/31 18:24:18 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:24:24 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:24:24 INFO Job: Job job_1626049283275_229527 completed successfully\n",
      "21/12/31 18:24:24 INFO Job: Counters: 33\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=265775\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5444818\n",
      "\t\tHDFS: Number of bytes written=2680738\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18200\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=3640\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3640\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18636800\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=27\n",
      "\t\tInput split bytes=159\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=28\n",
      "\t\tCPU time spent (ms)=2330\n",
      "\t\tPhysical memory (bytes) snapshot=311914496\n",
      "\t\tVirtual memory (bytes) snapshot=6388588544\n",
      "\t\tTotal committed heap usage (bytes)=1052770304\n",
      "\t\tPeak Map Physical memory (bytes)=311914496\n",
      "\t\tPeak Map Virtual memory (bytes)=6388588544\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680027\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2680738\n",
      "21/12/31 18:24:24 INFO MahoutDriver: Program took 52151 ms (Minutes: 0.8691833333333333)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout kmeans -i dsm010/british-fiction-corpus-vectors/tf-vectors -c dsm010/british-fiction-corpus-canopy-centroids -o hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans-clusters -dm org.apache.mahout.common.distance.CosineDistanceMeasure -cl -cd 0.1 -ow -x 20 -k 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   3 jfoul001 users        194 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/_policy\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/clusteredPoints\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 18:23 dsm010/british-fiction-corpus-kmeans-clusters/clusters-0\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 18:23 dsm010/british-fiction-corpus-kmeans-clusters/clusters-1\n",
      "drwxr-xr-x   - jfoul001 users          0 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-kmeans-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 jfoul001 users          0 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/clusteredPoints/_SUCCESS\n",
      "-rw-r--r--   3 jfoul001 users    2680738 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/clusteredPoints/part-m-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-kmeans-clusters/clusteredPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 jfoul001 users          0 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final/_SUCCESS\n",
      "-rw-r--r--   3 jfoul001 users        194 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final/_policy\n",
      "-rw-r--r--   3 jfoul001 users    2764438 2021-12-31 18:24 dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -ls dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Clustering Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the cluster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/31 18:47:33 INFO AbstractJob: Command line arguments: {--dictionary=[dsm010/british-fiction-corpus-vectors/dictionary.file-*], --dictionaryType=[sequencefile], --distanceMeasure=[org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure], --endPhase=[2147483647], --evaluate=null, --input=[dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final], --numWords=[20], --output=[/home/jfoul001/code/dsm010-2021-oct/coursework_01/data/output/british-fiction-corpus-clusters.txt], --outputFormat=[TEXT], --pointsDir=[dsm010/british-fiction-corpus-kmeans-clusters/clusteredPoints], --startPhase=[0], --substring=[100], --tempDir=[temp]}\n",
      "21/12/31 18:47:35 INFO HadoopUtil: Deleting tmp/representative\n",
      "21/12/31 18:47:35 INFO AbstractJob: Command line arguments: {--clusteredPoints=[dsm010/british-fiction-corpus-kmeans-clusters/clusteredPoints], --distanceMeasure=[org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure], --endPhase=[2147483647], --input=[dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final], --maxIter=[5], --method=[mapreduce], --output=[tmp/representative], --startPhase=[0], --tempDir=[temp]}\n",
      "21/12/31 18:47:35 INFO RepresentativePointsDriver: Representative Points Iteration 0\n",
      "21/12/31 18:47:35 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:47:35 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229769\n",
      "21/12/31 18:47:35 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:47:36 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:47:36 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229769\n",
      "21/12/31 18:47:36 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:47:36 INFO Configuration: resource-types.xml not found\n",
      "21/12/31 18:47:36 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/12/31 18:47:36 INFO YarnClientImpl: Submitted application application_1626049283275_229769\n",
      "21/12/31 18:47:36 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229769/\n",
      "21/12/31 18:47:36 INFO Job: Running job: job_1626049283275_229769\n",
      "21/12/31 18:47:42 INFO Job: Job job_1626049283275_229769 running in uber mode : false\n",
      "21/12/31 18:47:42 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:47:47 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:47:53 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:47:53 INFO Job: Job job_1626049283275_229769 completed successfully\n",
      "21/12/31 18:47:53 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1221703\n",
      "\t\tFILE: Number of bytes written=2975865\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5922766\n",
      "\t\tHDFS: Number of bytes written=2842735\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17115\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16900\n",
      "\t\tTotal time spent by all map tasks (ms)=3423\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3380\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3423\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3380\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17525760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17305600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=1221647\n",
      "\t\tMap output materialized bytes=1221703\n",
      "\t\tInput split bytes=172\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=1221703\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=20\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=34\n",
      "\t\tCPU time spent (ms)=3010\n",
      "\t\tPhysical memory (bytes) snapshot=696336384\n",
      "\t\tVirtual memory (bytes) snapshot=12764962816\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=396759040\n",
      "\t\tPeak Map Virtual memory (bytes)=6382579712\n",
      "\t\tPeak Reduce Physical memory (bytes)=299577344\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6386356224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680738\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2842735\n",
      "21/12/31 18:47:53 INFO RepresentativePointsDriver: Representative Points Iteration 1\n",
      "21/12/31 18:47:53 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:47:53 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229772\n",
      "21/12/31 18:47:53 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:47:53 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:47:53 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229772\n",
      "21/12/31 18:47:53 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:47:54 INFO YarnClientImpl: Submitted application application_1626049283275_229772\n",
      "21/12/31 18:47:54 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229772/\n",
      "21/12/31 18:47:54 INFO Job: Running job: job_1626049283275_229772\n",
      "21/12/31 18:48:00 INFO Job: Job job_1626049283275_229772 running in uber mode : false\n",
      "21/12/31 18:48:00 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:48:05 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:48:11 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:48:11 INFO Job: Job job_1626049283275_229772 completed successfully\n",
      "21/12/31 18:48:11 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1032400\n",
      "\t\tFILE: Number of bytes written=2597259\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8366380\n",
      "\t\tHDFS: Number of bytes written=3875240\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17015\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16665\n",
      "\t\tTotal time spent by all map tasks (ms)=3403\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3333\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3403\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3333\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17423360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17064960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=1032345\n",
      "\t\tMap output materialized bytes=1032400\n",
      "\t\tInput split bytes=172\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=1032400\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=30\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=58\n",
      "\t\tCPU time spent (ms)=3340\n",
      "\t\tPhysical memory (bytes) snapshot=708296704\n",
      "\t\tVirtual memory (bytes) snapshot=12789477376\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=396759040\n",
      "\t\tPeak Map Virtual memory (bytes)=6394249216\n",
      "\t\tPeak Reduce Physical memory (bytes)=311537664\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6395228160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680738\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3875240\n",
      "21/12/31 18:48:11 INFO RepresentativePointsDriver: Representative Points Iteration 2\n",
      "21/12/31 18:48:11 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:48:11 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229775\n",
      "21/12/31 18:48:11 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:48:11 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:48:11 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229775\n",
      "21/12/31 18:48:11 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:48:11 INFO YarnClientImpl: Submitted application application_1626049283275_229775\n",
      "21/12/31 18:48:11 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229775/\n",
      "21/12/31 18:48:11 INFO Job: Running job: job_1626049283275_229775\n",
      "21/12/31 18:48:17 INFO Job: Job job_1626049283275_229775 running in uber mode : false\n",
      "21/12/31 18:48:17 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:48:23 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:48:29 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:48:29 INFO Job: Job job_1626049283275_229775 completed successfully\n",
      "21/12/31 18:48:29 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1154054\n",
      "\t\tFILE: Number of bytes written=2840567\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10431390\n",
      "\t\tHDFS: Number of bytes written=5029358\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16975\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17560\n",
      "\t\tTotal time spent by all map tasks (ms)=3395\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3512\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3395\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3512\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17382400\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17981440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=1153998\n",
      "\t\tMap output materialized bytes=1154054\n",
      "\t\tInput split bytes=172\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=1154054\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=40\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=46\n",
      "\t\tCPU time spent (ms)=3430\n",
      "\t\tPhysical memory (bytes) snapshot=682156032\n",
      "\t\tVirtual memory (bytes) snapshot=12774772736\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=398155776\n",
      "\t\tPeak Map Virtual memory (bytes)=6376906752\n",
      "\t\tPeak Reduce Physical memory (bytes)=285548544\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6399361024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680738\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5029358\n",
      "21/12/31 18:48:29 INFO RepresentativePointsDriver: Representative Points Iteration 3\n",
      "21/12/31 18:48:29 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:48:29 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229778\n",
      "21/12/31 18:48:29 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:48:30 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:48:30 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229778\n",
      "21/12/31 18:48:30 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:48:30 INFO YarnClientImpl: Submitted application application_1626049283275_229778\n",
      "21/12/31 18:48:30 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229778/\n",
      "21/12/31 18:48:30 INFO Job: Running job: job_1626049283275_229778\n",
      "21/12/31 18:48:36 INFO Job: Job job_1626049283275_229778 running in uber mode : false\n",
      "21/12/31 18:48:36 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:48:41 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:48:47 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:48:47 INFO Job: Job job_1626049283275_229778 completed successfully\n",
      "21/12/31 18:48:47 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1123125\n",
      "\t\tFILE: Number of bytes written=2778709\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=12739626\n",
      "\t\tHDFS: Number of bytes written=6152587\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17405\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16990\n",
      "\t\tTotal time spent by all map tasks (ms)=3481\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3398\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3481\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3398\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17822720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17397760\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=1123069\n",
      "\t\tMap output materialized bytes=1123125\n",
      "\t\tInput split bytes=172\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=1123125\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=55\n",
      "\t\tCPU time spent (ms)=3750\n",
      "\t\tPhysical memory (bytes) snapshot=715169792\n",
      "\t\tVirtual memory (bytes) snapshot=12766113792\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=404238336\n",
      "\t\tPeak Map Virtual memory (bytes)=6376456192\n",
      "\t\tPeak Reduce Physical memory (bytes)=310931456\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6393622528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680738\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6152587\n",
      "21/12/31 18:48:47 INFO RepresentativePointsDriver: Representative Points Iteration 4\n",
      "21/12/31 18:48:47 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "21/12/31 18:48:47 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_229782\n",
      "21/12/31 18:48:47 INFO FileInputFormat: Total input files to process : 1\n",
      "21/12/31 18:48:47 INFO JobSubmitter: number of splits:1\n",
      "21/12/31 18:48:47 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_229782\n",
      "21/12/31 18:48:47 INFO JobSubmitter: Executing with tokens: []\n",
      "21/12/31 18:48:47 INFO YarnClientImpl: Submitted application application_1626049283275_229782\n",
      "21/12/31 18:48:47 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_229782/\n",
      "21/12/31 18:48:47 INFO Job: Running job: job_1626049283275_229782\n",
      "21/12/31 18:48:54 INFO Job: Job job_1626049283275_229782 running in uber mode : false\n",
      "21/12/31 18:48:54 INFO Job:  map 0% reduce 0%\n",
      "21/12/31 18:48:59 INFO Job:  map 100% reduce 0%\n",
      "21/12/31 18:49:05 INFO Job:  map 100% reduce 100%\n",
      "21/12/31 18:49:05 INFO Job: Job job_1626049283275_229782 completed successfully\n",
      "21/12/31 18:49:05 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1068026\n",
      "\t\tFILE: Number of bytes written=2668511\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=14986084\n",
      "\t\tHDFS: Number of bytes written=7220678\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17370\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16800\n",
      "\t\tTotal time spent by all map tasks (ms)=3474\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3360\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3474\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3360\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17786880\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17203200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=1067971\n",
      "\t\tMap output materialized bytes=1068026\n",
      "\t\tInput split bytes=172\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=1068026\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=60\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=48\n",
      "\t\tCPU time spent (ms)=3950\n",
      "\t\tPhysical memory (bytes) snapshot=759328768\n",
      "\t\tVirtual memory (bytes) snapshot=12756938752\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=436371456\n",
      "\t\tPeak Map Virtual memory (bytes)=6374313984\n",
      "\t\tPeak Reduce Physical memory (bytes)=322957312\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6382624768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2680738\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7220678\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Scaled Inter-Cluster Density = 0.24540640174678777\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[0] = 0.5834976196118247\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[24] = NaN\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[22] = 0.5089695027443909\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[11] = NaN\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[18] = 0.6399932478220899\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[17] = NaN\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[14] = 0.5324087672995395\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[19] = 0.5021463789065704\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[13] = NaN\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Intra-Cluster Density[9] = 0.4833333333333334\n",
      "21/12/31 18:49:05 INFO ClusterEvaluator: Average Intra-Cluster Density = 0.5417248082862914\n",
      "21/12/31 18:49:06 INFO ClusterDumper: Wrote 10 clusters\n",
      "21/12/31 18:49:06 INFO MahoutDriver: Program took 92961 ms (Minutes: 1.54935)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout clusterdump -dt sequencefile \\\n",
    "    -d dsm010/british-fiction-corpus-vectors/dictionary.file-* \\\n",
    "    -i dsm010/british-fiction-corpus-kmeans-clusters/clusters-2-final  \\\n",
    "    -o ~/code/dsm010-2021-oct/coursework_01/data/output/british-fiction-corpus-clusters.txt -b 100 \\\n",
    "    -p dsm010/british-fiction-corpus-kmeans-clusters/clusteredPoints \\\n",
    "    -n 20 --evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Cluster Dump Output\n",
    "\n",
    "The final lines give a statistical evaluation of how good the clustering solution was. You can disregard the CDbw metries. Low inter-cluster density and high intra-cluster density indicates a good solution. The beginning of the file gives representative members (terms) in each cluster, as well as the actual documents in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Cluster Density: 0.24540640174678777\n",
      "Intra-Cluster Density: 0.5417248082862914\n",
      "CDbw Inter-Cluster Density: 0.0\n",
      "CDbw Intra-Cluster Density: 0.8205571850805228\n",
      "CDbw Separation: 6.966629158888863E8\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tail -n 5 ~/code/dsm010-2021-oct/coursework_01/data/output/british-fiction-corpus-clusters.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Distance Metric: org.apache.mahout.common.distance.ManhattanDistanceMeasure\n",
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n",
      "---- K: 03 -- dsm010/british-fiction-corpus-kmeans/ManhattanDistanceMeasure/03\n",
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n",
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/01 12:53:39 INFO AbstractJob: Command line arguments: {--distanceMeasure=[org.apache.mahout.common.distance.ManhattanDistanceMeasure], --endPhase=[2147483647], --input=[dsm010/british-fiction-corpus-vectors/tf-vectors], --method=[mapreduce], --output=[dsm010/british-fiction-corpus-canopy-centroids], --overwrite=null, --startPhase=[0], --t1=[0.5], --t2=[0.3], --tempDir=[temp]}\n",
      "22/01/01 12:53:40 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-canopy-centroids\n",
      "22/01/01 12:53:40 INFO CanopyDriver: Build Clusters Input: dsm010/british-fiction-corpus-vectors/tf-vectors Out: dsm010/british-fiction-corpus-canopy-centroids Measure: org.apache.mahout.common.distance.ManhattanDistanceMeasure@4ade6987 t1: 0.5 t2: 0.3\n",
      "22/01/01 12:53:40 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:53:40 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238975\n",
      "22/01/01 12:53:40 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:53:41 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:53:41 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238975\n",
      "22/01/01 12:53:41 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:53:41 INFO Configuration: resource-types.xml not found\n",
      "22/01/01 12:53:41 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/01/01 12:53:41 INFO YarnClientImpl: Submitted application application_1626049283275_238975\n",
      "22/01/01 12:53:41 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238975/\n",
      "22/01/01 12:53:41 INFO Job: Running job: job_1626049283275_238975\n",
      "22/01/01 12:53:47 INFO Job: Job job_1626049283275_238975 running in uber mode : false\n",
      "22/01/01 12:53:47 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:53:54 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:54:01 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:54:01 INFO Job: Job job_1626049283275_238975 completed successfully\n",
      "22/01/01 12:54:01 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2916602\n",
      "\t\tFILE: Number of bytes written=6366839\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837347\n",
      "\t\tHDFS: Number of bytes written=2920999\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25570\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19925\n",
      "\t\tTotal time spent by all map tasks (ms)=5114\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3985\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5114\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3985\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=26183680\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=20403200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=27\n",
      "\t\tMap output bytes=2916466\n",
      "\t\tMap output materialized bytes=2916602\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=2916602\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=27\n",
      "\t\tSpilled Records=54\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=83\n",
      "\t\tCPU time spent (ms)=7360\n",
      "\t\tPhysical memory (bytes) snapshot=882003968\n",
      "\t\tVirtual memory (bytes) snapshot=12774887424\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=515530752\n",
      "\t\tPeak Map Virtual memory (bytes)=6384730112\n",
      "\t\tPeak Reduce Physical memory (bytes)=366473216\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6390157312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2920999\n",
      "22/01/01 12:54:01 INFO MahoutDriver: Program took 21863 ms (Minutes: 0.36438333333333334)\n",
      "22/01/01 12:54:06 INFO AbstractJob: Command line arguments: {--clustering=null, --clusters=[dsm010/british-fiction-corpus-canopy-centroids], --convergenceDelta=[0.1], --distanceMeasure=[org.apache.mahout.common.distance.ManhattanDistanceMeasure], --endPhase=[2147483647], --input=[dsm010/british-fiction-corpus-vectors/tf-vectors], --maxIter=[20], --method=[mapreduce], --numClusters=[3], --output=[hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans/ManhattanDistanceMeasure/03], --overwrite=null, --startPhase=[0], --tempDir=[temp]}\n",
      "22/01/01 12:54:06 INFO HadoopUtil: Deleting dsm010/british-fiction-corpus-canopy-centroids\n",
      "22/01/01 12:54:06 INFO ZlibFactory: Successfully loaded & initialized native-zlib library\n",
      "22/01/01 12:54:06 INFO CodecPool: Got brand-new compressor [.deflate]\n",
      "22/01/01 12:54:07 INFO RandomSeedGenerator: Wrote 3 Klusters to dsm010/british-fiction-corpus-canopy-centroids/part-randomSeed\n",
      "22/01/01 12:54:07 INFO KMeansDriver: Input: dsm010/british-fiction-corpus-vectors/tf-vectors Clusters In: dsm010/british-fiction-corpus-canopy-centroids/part-randomSeed Out: hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans/ManhattanDistanceMeasure/03\n",
      "22/01/01 12:54:07 INFO KMeansDriver: convergence: 0.1 max Iterations: 20\n",
      "22/01/01 12:54:07 INFO CodecPool: Got brand-new decompressor [.deflate]\n",
      "22/01/01 12:54:07 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:54:07 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238979\n",
      "22/01/01 12:54:07 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:54:07 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:54:07 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238979\n",
      "22/01/01 12:54:07 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:54:07 INFO Configuration: resource-types.xml not found\n",
      "22/01/01 12:54:07 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/01/01 12:54:08 INFO YarnClientImpl: Submitted application application_1626049283275_238979\n",
      "22/01/01 12:54:08 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238979/\n",
      "22/01/01 12:54:08 INFO Job: Running job: job_1626049283275_238979\n",
      "22/01/01 12:54:14 INFO Job: Job job_1626049283275_238979 running in uber mode : false\n",
      "22/01/01 12:54:14 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:54:20 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:54:26 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:54:26 INFO Job: Job job_1626049283275_238979 completed successfully\n",
      "22/01/01 12:54:26 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2164776\n",
      "\t\tFILE: Number of bytes written=4861735\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8045627\n",
      "\t\tHDFS: Number of bytes written=1640105\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18760\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17730\n",
      "\t\tTotal time spent by all map tasks (ms)=3752\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3546\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3752\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3546\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19210240\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18155520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=3\n",
      "\t\tMap output bytes=2164755\n",
      "\t\tMap output materialized bytes=2164776\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=2164776\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=6\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=43\n",
      "\t\tCPU time spent (ms)=4610\n",
      "\t\tPhysical memory (bytes) snapshot=786493440\n",
      "\t\tVirtual memory (bytes) snapshot=12775448576\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=438095872\n",
      "\t\tPeak Map Virtual memory (bytes)=6383280128\n",
      "\t\tPeak Reduce Physical memory (bytes)=348397568\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6392168448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1640105\n",
      "22/01/01 12:54:26 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:54:26 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238982\n",
      "22/01/01 12:54:26 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:54:26 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:54:26 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238982\n",
      "22/01/01 12:54:26 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:54:26 INFO YarnClientImpl: Submitted application application_1626049283275_238982\n",
      "22/01/01 12:54:26 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238982/\n",
      "22/01/01 12:54:26 INFO Job: Running job: job_1626049283275_238982\n",
      "22/01/01 12:54:32 INFO Job: Job job_1626049283275_238982 running in uber mode : false\n",
      "22/01/01 12:54:32 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:54:39 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:54:45 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:54:45 INFO Job: Job job_1626049283275_238982 completed successfully\n",
      "22/01/01 12:54:45 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3436279\n",
      "\t\tFILE: Number of bytes written=7404741\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9117945\n",
      "\t\tHDFS: Number of bytes written=1640105\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19150\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17835\n",
      "\t\tTotal time spent by all map tasks (ms)=3830\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3567\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3830\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3567\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19609600\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18263040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=3\n",
      "\t\tMap output bytes=3436258\n",
      "\t\tMap output materialized bytes=3436279\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=3436279\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=6\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=87\n",
      "\t\tCPU time spent (ms)=4780\n",
      "\t\tPhysical memory (bytes) snapshot=724475904\n",
      "\t\tVirtual memory (bytes) snapshot=12766863360\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=412684288\n",
      "\t\tPeak Map Virtual memory (bytes)=6380740608\n",
      "\t\tPeak Reduce Physical memory (bytes)=311791616\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6386122752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1640105\n",
      "22/01/01 12:54:46 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:54:46 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238985\n",
      "22/01/01 12:54:46 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:54:46 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:54:46 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238985\n",
      "22/01/01 12:54:46 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:54:46 INFO YarnClientImpl: Submitted application application_1626049283275_238985\n",
      "22/01/01 12:54:46 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238985/\n",
      "22/01/01 12:54:46 INFO Job: Running job: job_1626049283275_238985\n",
      "22/01/01 12:54:52 INFO Job: Job job_1626049283275_238985 running in uber mode : false\n",
      "22/01/01 12:54:52 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:54:59 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:55:05 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 12:55:05 INFO Job: Job job_1626049283275_238985 completed successfully\n",
      "22/01/01 12:55:05 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3436279\n",
      "\t\tFILE: Number of bytes written=7404741\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9117945\n",
      "\t\tHDFS: Number of bytes written=1640105\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19235\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17185\n",
      "\t\tTotal time spent by all map tasks (ms)=3847\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3437\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3847\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3437\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19696640\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17597440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=3\n",
      "\t\tMap output bytes=3436258\n",
      "\t\tMap output materialized bytes=3436279\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=3436279\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=6\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=54\n",
      "\t\tCPU time spent (ms)=4550\n",
      "\t\tPhysical memory (bytes) snapshot=759087104\n",
      "\t\tVirtual memory (bytes) snapshot=12774772736\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=421273600\n",
      "\t\tPeak Map Virtual memory (bytes)=6383648768\n",
      "\t\tPeak Reduce Physical memory (bytes)=337813504\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6391123968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1640105\n",
      "22/01/01 12:55:05 INFO KMeansDriver: Clustering data\n",
      "22/01/01 12:55:05 INFO KMeansDriver: Running Clustering\n",
      "22/01/01 12:55:05 INFO KMeansDriver: Input: dsm010/british-fiction-corpus-vectors/tf-vectors Clusters In: hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans/ManhattanDistanceMeasure/03 Out: hdfs://lena/user/jfoul001/dsm010/british-fiction-corpus-kmeans/ManhattanDistanceMeasure/03\n",
      "22/01/01 12:55:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 12:55:05 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_238988\n",
      "22/01/01 12:55:05 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 12:55:05 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 12:55:06 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_238988\n",
      "22/01/01 12:55:06 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 12:55:06 INFO YarnClientImpl: Submitted application application_1626049283275_238988\n",
      "22/01/01 12:55:06 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_238988/\n",
      "22/01/01 12:55:06 INFO Job: Running job: job_1626049283275_238988\n",
      "22/01/01 12:55:12 INFO Job: Job job_1626049283275_238988 running in uber mode : false\n",
      "22/01/01 12:55:12 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 12:55:18 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 12:55:18 INFO Job: Job job_1626049283275_238988 completed successfully\n",
      "22/01/01 12:55:18 INFO Job: Counters: 33\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=265832\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7477646\n",
      "\t\tHDFS: Number of bytes written=5837857\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19440\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=3888\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3888\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19906560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=54\n",
      "\t\tMap output records=54\n",
      "\t\tInput split bytes=159\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tCPU time spent (ms)=2830\n",
      "\t\tPhysical memory (bytes) snapshot=307892224\n",
      "\t\tVirtual memory (bytes) snapshot=6387339264\n",
      "\t\tTotal committed heap usage (bytes)=1052770304\n",
      "\t\tPeak Map Physical memory (bytes)=307892224\n",
      "\t\tPeak Map Virtual memory (bytes)=6387339264\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837188\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837857\n",
      "22/01/01 12:55:18 INFO MahoutDriver: Program took 72370 ms (Minutes: 1.2061666666666666)\n",
      "22/01/01 12:55:22 INFO AbstractJob: Command line arguments: {--dictionary=[dsm010/british-fiction-corpus-vectors/dictionary.file-*], --dictionaryType=[sequencefile], --distanceMeasure=[org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure], --endPhase=[2147483647], --input=[dsm010/british-fiction-corpus-kmeans/ManhattanDistanceMeasure/03/clusters-2-final], --numWords=[20], --output=[/home/jfoul001/code/dsm010-2021-oct/coursework_01/data/output/british-fiction-corpus-clusters/ManhattanDistanceMeasure/03.txt], --outputFormat=[JSON], --pointsDir=[dsm010/british-fiction-corpus-kmeans/ManhattanDistanceMeasure/03/clusteredPoints], --startPhase=[0], --substring=[100], --tempDir=[temp]}\n",
      "22/01/01 12:55:24 INFO ClusterDumper: Wrote 0 clusters\n",
      "22/01/01 12:55:24 INFO MahoutDriver: Program took 1056 ms (Minutes: 0.0176)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "# range to use for k\n",
    "k_start=3\n",
    "k_end=3\n",
    "\n",
    "# the path to the vectors, centroids and dictionary\n",
    "path_vectors=dsm010/british-fiction-corpus-vectors/tf-vectors\n",
    "path_centroids=dsm010/british-fiction-corpus-canopy-centroids\n",
    "path_dictionary=dsm010/british-fiction-corpus-vectors/dictionary.file-*\n",
    "\n",
    "# the output base path for the clusters and the result local output path\n",
    "path_hdfs_base=hdfs://lena/user/jfoul001/\n",
    "path_clusters_base=dsm010/british-fiction-corpus-kmeans\n",
    "path_results_base=~/code/dsm010-2021-oct/coursework_01/data/output/british-fiction-corpus-clusters\n",
    "\n",
    "# the distance metric to use\n",
    "#distance_metrics=(\"org.apache.mahout.common.distance.CosineDistanceMeasure\" \"org.apache.mahout.common.distance.EuclideanDistanceMeasure\" \"org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure\" \"org.apache.mahout.common.distance.ManhattanDistanceMeasure\")\n",
    "distance_metrics=(\"org.apache.mahout.common.distance.CosineDistanceMeasure\")\n",
    "\n",
    "for distance_metric in \"${distance_metrics[@]}\"\n",
    "do\n",
    "  echo \"--- Distance Metric: $distance_metric\"\n",
    "\n",
    "    # perform the canopy clustering\n",
    "    mahout canopy \\\n",
    "      -i $path_vectors \\\n",
    "      -ow \\\n",
    "      -o $path_centroids \\\n",
    "      -dm $distance_metric \\\n",
    "      -t1 0.5 \\\n",
    "      -t2 0.3\n",
    "\n",
    "  for ((k = $k_start; k <= $k_end; k++))\n",
    "  do\n",
    "    # get k with a leading zero if required\n",
    "    k_padded=$(printf %02d $k)\n",
    "\n",
    "    # set the output path for the clusters\n",
    "    distance_name=${distance_metric##*.}\n",
    "    path_clusters=\"${path_clusters_base}/${distance_name}/${k_padded}\"\n",
    "\n",
    "    echo \"---- K: $k_padded -- $path_clusters\"\n",
    "\n",
    "    # perform the kmeans clustering\n",
    "    mahout kmeans \\\n",
    "    -i $path_vectors \\\n",
    "    -c $path_centroids \\\n",
    "    -o \"${path_hdfs_base}${path_clusters}\" \\\n",
    "    -ow \\\n",
    "    -dm $distance_metric \\\n",
    "    -cl -cd 0.1 -ow -x 20 \\\n",
    "    -k $k\n",
    "\n",
    "    # set the path for output\n",
    "    path_final_clusters=\"${path_clusters}/clusters-2-final\"\n",
    "    path_clusterpoints=\"${path_clusters}/clusteredPoints\"\n",
    "    path_results=\"${path_results_base}/${distance_name}/${k_padded}.txt\"\n",
    "\n",
    "    # output the cluster results\n",
    "    # mahout clusterdump -dt sequencefile \\\n",
    "    #    -d $path_dictionary \\\n",
    "    #    -i $path_final_clusters  \\\n",
    "    #    -o $path_results \\\n",
    "    #    -of TEXT \\\n",
    "    #    -b 100 \\\n",
    "    #    -p $path_clusterpoints \\\n",
    "    #    -dm $distance_metric \\\n",
    "    #    -n 20 --evaluate\n",
    "        mahout clusterdump -dt sequencefile \\\n",
    "       -d $path_dictionary \\\n",
    "       -i $path_final_clusters  \\\n",
    "       -o $path_results \\\n",
    "       -of JSON \\\n",
    "       -b 100 \\\n",
    "       -p $path_clusterpoints \\\n",
    "       -n 20\n",
    "  done  \n",
    "done"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "429f3e8d45833d845e6e031dbf3e229703adf0bd2129019c99d8da7ba46dd29e"
  },
  "kernelspec": {
   "display_name": "Bash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
