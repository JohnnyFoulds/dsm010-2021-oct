{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1: Question 2 - Vodacom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data\n",
    "\n",
    "Get the source data to use for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load\n",
    "\n",
    "Load customers reviews that was previously downloaded from Hellopeter in the [DSM020](https://github.com/JohnnyFoulds/dsm020-2021-oct) git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35072, 17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source = pd.read_parquet('https://github.com/JohnnyFoulds/dsm020-2021-oct/blob/master/coursework_01/data/output/telecommunications.gzip?raw=true')\n",
    "df_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_content</th>\n",
       "      <th>business_slug</th>\n",
       "      <th>permalink</th>\n",
       "      <th>replied</th>\n",
       "      <th>messages</th>\n",
       "      <th>industry_slug</th>\n",
       "      <th>nps_rating</th>\n",
       "      <th>author_created_date</th>\n",
       "      <th>author_total_reviews_count</th>\n",
       "      <th>review_title_clean</th>\n",
       "      <th>review_content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3750417</td>\n",
       "      <td>2021-12-21 15:27:08</td>\n",
       "      <td>Barry S</td>\n",
       "      <td>05c6e290-6186-11ec-b1c0-cd74559df45d</td>\n",
       "      <td>Worst Service</td>\n",
       "      <td>1</td>\n",
       "      <td>Worst service I’ve received in my life. Non of...</td>\n",
       "      <td>vodacom</td>\n",
       "      <td>worst-service-04099ff193227e2908f31413b00ffb30...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>telecommunications</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>1</td>\n",
       "      <td>Worst Service</td>\n",
       "      <td>Worst service I’ve received in my life. Non of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3750406</td>\n",
       "      <td>2021-12-21 15:19:45</td>\n",
       "      <td>Kulani Marry-Aan</td>\n",
       "      <td>0a616360-6260-11ec-b18f-0f0735b462a0</td>\n",
       "      <td>Complaint about customer service</td>\n",
       "      <td>1</td>\n",
       "      <td>18/12/2021 i bought a router at Vodacom Bushbu...</td>\n",
       "      <td>vodacom</td>\n",
       "      <td>complaint-about-customer-service-28c9107344f3a...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>telecommunications</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Complaint about customer service</td>\n",
       "      <td>18/12/2021 i bought a router at Vodacom Bushbu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          created_at            author  \\\n",
       "0  3750417 2021-12-21 15:27:08           Barry S   \n",
       "1  3750406 2021-12-21 15:19:45  Kulani Marry-Aan   \n",
       "\n",
       "                              author_id                      review_title  \\\n",
       "0  05c6e290-6186-11ec-b1c0-cd74559df45d                     Worst Service   \n",
       "1  0a616360-6260-11ec-b18f-0f0735b462a0  Complaint about customer service   \n",
       "\n",
       "   review_rating                                     review_content  \\\n",
       "0              1  Worst service I’ve received in my life. Non of...   \n",
       "1              1  18/12/2021 i bought a router at Vodacom Bushbu...   \n",
       "\n",
       "  business_slug                                          permalink  replied  \\\n",
       "0       vodacom  worst-service-04099ff193227e2908f31413b00ffb30...    False   \n",
       "1       vodacom  complaint-about-customer-service-28c9107344f3a...    False   \n",
       "\n",
       "  messages       industry_slug  nps_rating author_created_date  \\\n",
       "0       []  telecommunications         NaN          2021-12-20   \n",
       "1       []  telecommunications         NaN          2021-12-21   \n",
       "\n",
       "   author_total_reviews_count                review_title_clean  \\\n",
       "0                           1                     Worst Service   \n",
       "1                           1  Complaint about customer service   \n",
       "\n",
       "                                review_content_clean  \n",
       "0  Worst service I’ve received in my life. Non of...  \n",
       "1  18/12/2021 i bought a router at Vodacom Bushbu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "Select only reviews from `vodacom` with the `review_title` and `review_content_clean` being the only data of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13699, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews = df_source.query('business_slug == \"vodacom\"')[['id', 'review_title', 'review_content_clean']].reset_index(drop=True)\n",
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3750417</td>\n",
       "      <td>Worst Service</td>\n",
       "      <td>Worst service I’ve received in my life. Non of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3750406</td>\n",
       "      <td>Complaint about customer service</td>\n",
       "      <td>18/12/2021 i bought a router at Vodacom Bushbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3750373</td>\n",
       "      <td>VODACOM not living up to promises and just wan...</td>\n",
       "      <td>I need help with Vodacom.  I have been a custo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       review_title  \\\n",
       "0  3750417                                      Worst Service   \n",
       "1  3750406                   Complaint about customer service   \n",
       "2  3750373  VODACOM not living up to promises and just wan...   \n",
       "\n",
       "                                review_content_clean  \n",
       "0  Worst service I’ve received in my life. Non of...  \n",
       "1  18/12/2021 i bought a router at Vodacom Bushbu...  \n",
       "2  I need help with Vodacom.  I have been a custo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Create an individual text file for each review for upload to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf720a4cbdc4a48bdee05518099342a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_files(data:pd.DataFrame, output_path:str, id_column:str='id', title_column:str='review_title', content_column:str='review_content_clean') -> None:\n",
    "    \"\"\"\n",
    "    Create a output .txt file for each review in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The DataFrame containint the reviews to process.\n",
    "\n",
    "    output_path : str\n",
    "        The path where the .txt files should be stored\n",
    "\n",
    "    id_column : str\n",
    "        The name of the ID column that will be used for the filename.\n",
    "\n",
    "    title_column : str\n",
    "        The name of the column in the DataFrame containing the review title.\n",
    "\n",
    "    content_column : str\n",
    "        The name of the column in the DataFrame containing the review text.\n",
    "    \"\"\"\n",
    "    for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        # get the output filename\n",
    "        filename = '%s/%s.txt' % (output_path, row[id_column])\n",
    "\n",
    "        # get the text to write to file\n",
    "        output_text = '%s\\n\\n%s' % (row[title_column], row[content_column])\n",
    "\n",
    "        # create the output file\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(output_text)\n",
    "\n",
    "# create the output files \n",
    "sample_size = 0.6\n",
    "df_sample = df_reviews.sample(frac=sample_size, random_state=3231)\n",
    "create_files(df_sample, 'data/input/vodacom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data Files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted dsm010/vodacom-corpus\n",
      "Found 8216 items\n",
      "-rw-r--r--   3 jfoul001 users        496 2022-01-01 20:48 dsm010/vodacom-corpus/3337933.txt\n",
      "-rw-r--r--   3 jfoul001 users        269 2022-01-01 20:48 dsm010/vodacom-corpus/3337940.txt\n",
      "-rw-r--r--   3 jfoul001 users        268 2022-01-01 20:48 dsm010/vodacom-corpus/3337941.txt\n",
      "-rw-r--r--   3 jfoul001 users        578 2022-01-01 20:47 dsm010/vodacom-corpus/3337954.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# change to the coursework directory\n",
    "cd ~/code/dsm010-2021-oct/coursework_01/\n",
    "\n",
    "# delete existing files\n",
    "hadoop fs -rm -r dsm010/vodacom-corpus\n",
    "\n",
    "# copy the input documents\n",
    " hadoop fs -copyFromLocal data/input/vodacom dsm010/vodacom-corpus\n",
    "\n",
    "# verify the file uploads\n",
    "hadoop fs -ls dsm010/vodacom-corpus | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset to SequenceFiles\n",
    "\n",
    "SequenceFiles are flat files consisting of binary key/value pairs. Each document is represented as a key-value pair. there the key is the document id and value is its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/01 20:49:07 INFO AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[5], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[dsm010/vodacom-corpus], --keyPrefix=[], --method=[mapreduce], --output=[dsm010/vodacom-corpus-seqfiles], --overwrite=null, --startPhase=[0], --tempDir=[temp]}\n",
      "22/01/01 20:49:08 INFO HadoopUtil: Deleting dsm010/vodacom-corpus-seqfiles\n",
      "22/01/01 20:49:08 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n",
      "22/01/01 20:49:08 INFO deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n",
      "22/01/01 20:49:08 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "22/01/01 20:49:08 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:49:08 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242074\n",
      "22/01/01 20:49:09 INFO FileInputFormat: Total input files to process : 8216\n",
      "22/01/01 20:49:10 INFO JobSubmitter: number of splits:2\n",
      "22/01/01 20:49:10 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242074\n",
      "22/01/01 20:49:10 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:49:10 INFO Configuration: resource-types.xml not found\n",
      "22/01/01 20:49:10 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/01/01 20:49:10 INFO YarnClientImpl: Submitted application application_1626049283275_242074\n",
      "22/01/01 20:49:10 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242074/\n",
      "22/01/01 20:49:10 INFO Job: Running job: job_1626049283275_242074\n",
      "22/01/01 20:49:16 INFO Job: Job job_1626049283275_242074 running in uber mode : false\n",
      "22/01/01 20:49:16 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:49:25 INFO Job:  map 50% reduce 0%\n",
      "22/01/01 20:49:33 INFO Job:  map 87% reduce 0%\n",
      "22/01/01 20:49:37 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:49:38 INFO Job: Job job_1626049283275_242074 completed successfully\n",
      "22/01/01 20:49:38 INFO Job: Counters: 33\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=531022\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7116808\n",
      "\t\tHDFS: Number of bytes written=3711172\n",
      "\t\tHDFS: Number of read operations=32876\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=132150\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=26430\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=26430\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=135321600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8216\n",
      "\t\tMap output records=8216\n",
      "\t\tInput split bytes=682078\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=79\n",
      "\t\tCPU time spent (ms)=28660\n",
      "\t\tPhysical memory (bytes) snapshot=773685248\n",
      "\t\tVirtual memory (bytes) snapshot=12776689664\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=443629568\n",
      "\t\tPeak Map Virtual memory (bytes)=6394564608\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3711172\n",
      "22/01/01 20:49:38 INFO MahoutDriver: Program took 30240 ms (Minutes: 0.504)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout seqdirectory \\\n",
    "    -i dsm010/vodacom-corpus \\\n",
    "    -o dsm010/vodacom-corpus-seqfiles \\\n",
    "    -ow \\\n",
    "    -c UTF-8 \\\n",
    "    -chunk 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sequenceFiles to sparse vector files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\n",
      "Running on hadoop, using /opt/hadoop/current/bin/hadoop and HADOOP_CONF_DIR=/opt/hadoop/current/etc/hadoop\n",
      "MAHOUT-JOB: /opt/mahout/current/mahout-examples-0.13.0-job.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/01 20:49:44 INFO SparseVectorsFromSequenceFiles: Maximum n-gram size is: 1\n",
      "22/01/01 20:49:44 INFO HadoopUtil: Deleting dsm010/vodacom-corpus-vectors\n",
      "22/01/01 20:49:44 INFO SparseVectorsFromSequenceFiles: Minimum LLR value: 1.0\n",
      "22/01/01 20:49:44 INFO SparseVectorsFromSequenceFiles: Number of reduce tasks: 1\n",
      "22/01/01 20:49:44 INFO SparseVectorsFromSequenceFiles: Tokenizing documents in dsm010/vodacom-corpus-seqfiles\n",
      "22/01/01 20:49:44 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:49:45 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242076\n",
      "22/01/01 20:49:45 INFO FileInputFormat: Total input files to process : 2\n",
      "22/01/01 20:49:45 INFO JobSubmitter: number of splits:2\n",
      "22/01/01 20:49:45 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242076\n",
      "22/01/01 20:49:45 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:49:45 INFO Configuration: resource-types.xml not found\n",
      "22/01/01 20:49:45 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/01/01 20:49:45 INFO YarnClientImpl: Submitted application application_1626049283275_242076\n",
      "22/01/01 20:49:45 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242076/\n",
      "22/01/01 20:49:45 INFO Job: Running job: job_1626049283275_242076\n",
      "22/01/01 20:49:52 INFO Job: Job job_1626049283275_242076 running in uber mode : false\n",
      "22/01/01 20:49:52 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:49:58 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:49:58 INFO Job: Job job_1626049283275_242076 completed successfully\n",
      "22/01/01 20:49:58 INFO Job: Counters: 34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=530116\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3711454\n",
      "\t\tHDFS: Number of bytes written=5151634\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=34205\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=6841\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6841\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35025920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8216\n",
      "\t\tMap output records=8216\n",
      "\t\tInput split bytes=282\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=55\n",
      "\t\tCPU time spent (ms)=5230\n",
      "\t\tPhysical memory (bytes) snapshot=678342656\n",
      "\t\tVirtual memory (bytes) snapshot=12783730688\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=378535936\n",
      "\t\tPeak Map Virtual memory (bytes)=6397095936\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3711172\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5151634\n",
      "22/01/01 20:49:58 INFO SparseVectorsFromSequenceFiles: Creating Term Frequency Vectors\n",
      "22/01/01 20:49:58 INFO DictionaryVectorizer: Creating dictionary from dsm010/vodacom-corpus-vectors/tokenized-documents and saving at dsm010/vodacom-corpus-vectors/wordcount\n",
      "22/01/01 20:49:58 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:49:58 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242077\n",
      "22/01/01 20:49:58 INFO FileInputFormat: Total input files to process : 2\n",
      "22/01/01 20:49:58 INFO JobSubmitter: number of splits:2\n",
      "22/01/01 20:49:58 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242077\n",
      "22/01/01 20:49:58 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:49:58 INFO YarnClientImpl: Submitted application application_1626049283275_242077\n",
      "22/01/01 20:49:58 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242077/\n",
      "22/01/01 20:49:58 INFO Job: Running job: job_1626049283275_242077\n",
      "22/01/01 20:50:04 INFO Job: Job job_1626049283275_242077 running in uber mode : false\n",
      "22/01/01 20:50:04 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:50:10 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:50:14 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:50:14 INFO Job: Job job_1626049283275_242077 completed successfully\n",
      "22/01/01 20:50:14 INFO Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=495284\n",
      "\t\tFILE: Number of bytes written=1787340\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5151954\n",
      "\t\tHDFS: Number of bytes written=252118\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36805\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8830\n",
      "\t\tTotal time spent by all map tasks (ms)=7361\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1766\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7361\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1766\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=37688320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9041920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8216\n",
      "\t\tMap output records=562590\n",
      "\t\tMap output bytes=8078314\n",
      "\t\tMap output materialized bytes=495290\n",
      "\t\tInput split bytes=320\n",
      "\t\tCombine input records=562590\n",
      "\t\tCombine output records=27720\n",
      "\t\tReduce input groups=21299\n",
      "\t\tReduce shuffle bytes=495290\n",
      "\t\tReduce input records=27720\n",
      "\t\tReduce output records=10638\n",
      "\t\tSpilled Records=55440\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=81\n",
      "\t\tCPU time spent (ms)=8720\n",
      "\t\tPhysical memory (bytes) snapshot=1217781760\n",
      "\t\tVirtual memory (bytes) snapshot=19201581056\n",
      "\t\tTotal committed heap usage (bytes)=3158310912\n",
      "\t\tPeak Map Physical memory (bytes)=473927680\n",
      "\t\tPeak Map Virtual memory (bytes)=6398095360\n",
      "\t\tPeak Reduce Physical memory (bytes)=340951040\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6409445376\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5151634\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=252118\n",
      "22/01/01 20:50:15 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:50:15 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242079\n",
      "22/01/01 20:50:15 INFO FileInputFormat: Total input files to process : 2\n",
      "22/01/01 20:50:15 INFO JobSubmitter: number of splits:2\n",
      "22/01/01 20:50:15 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242079\n",
      "22/01/01 20:50:15 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:50:15 INFO YarnClientImpl: Submitted application application_1626049283275_242079\n",
      "22/01/01 20:50:15 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242079/\n",
      "22/01/01 20:50:15 INFO Job: Running job: job_1626049283275_242079\n",
      "22/01/01 20:50:21 INFO Job: Job job_1626049283275_242079 running in uber mode : false\n",
      "22/01/01 20:50:21 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:50:26 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:50:32 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:50:33 INFO Job: Job job_1626049283275_242079 completed successfully\n",
      "22/01/01 20:50:34 INFO Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5115793\n",
      "\t\tFILE: Number of bytes written=11033458\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5361519\n",
      "\t\tHDFS: Number of bytes written=5837807\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32645\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18375\n",
      "\t\tTotal time spent by all map tasks (ms)=6529\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3675\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6529\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3675\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=33428480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18816000\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8216\n",
      "\t\tMap output records=8216\n",
      "\t\tMap output bytes=5084748\n",
      "\t\tMap output materialized bytes=5115799\n",
      "\t\tInput split bytes=320\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8216\n",
      "\t\tReduce shuffle bytes=5115799\n",
      "\t\tReduce input records=8216\n",
      "\t\tReduce output records=8215\n",
      "\t\tSpilled Records=16432\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=69\n",
      "\t\tCPU time spent (ms)=7390\n",
      "\t\tPhysical memory (bytes) snapshot=1193000960\n",
      "\t\tVirtual memory (bytes) snapshot=19170181120\n",
      "\t\tTotal committed heap usage (bytes)=3158310912\n",
      "\t\tPeak Map Physical memory (bytes)=436387840\n",
      "\t\tPeak Map Virtual memory (bytes)=6390185984\n",
      "\t\tPeak Reduce Physical memory (bytes)=360804352\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6406082560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tTFPartialVectorReducer\n",
      "\t\temptyVectorCount=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5151634\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837807\n",
      "22/01/01 20:50:34 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:50:34 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242081\n",
      "22/01/01 20:50:34 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 20:50:34 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 20:50:34 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242081\n",
      "22/01/01 20:50:34 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:50:34 INFO YarnClientImpl: Submitted application application_1626049283275_242081\n",
      "22/01/01 20:50:34 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242081/\n",
      "22/01/01 20:50:34 INFO Job: Running job: job_1626049283275_242081\n",
      "22/01/01 20:50:40 INFO Job: Job job_1626049283275_242081 running in uber mode : false\n",
      "22/01/01 20:50:40 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:50:45 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:50:51 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:50:52 INFO Job: Job job_1626049283275_242081 completed successfully\n",
      "22/01/01 20:50:52 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5802947\n",
      "\t\tFILE: Number of bytes written=12137765\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837965\n",
      "\t\tHDFS: Number of bytes written=5837807\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16810\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16710\n",
      "\t\tTotal time spent by all map tasks (ms)=3362\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3342\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3362\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3342\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17213440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17111040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8215\n",
      "\t\tMap output records=8215\n",
      "\t\tMap output bytes=5770877\n",
      "\t\tMap output materialized bytes=5802947\n",
      "\t\tInput split bytes=158\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8215\n",
      "\t\tReduce shuffle bytes=5802947\n",
      "\t\tReduce input records=8215\n",
      "\t\tReduce output records=8215\n",
      "\t\tSpilled Records=16430\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=56\n",
      "\t\tCPU time spent (ms)=5160\n",
      "\t\tPhysical memory (bytes) snapshot=802770944\n",
      "\t\tVirtual memory (bytes) snapshot=12814934016\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=438452224\n",
      "\t\tPeak Map Virtual memory (bytes)=6398803968\n",
      "\t\tPeak Reduce Physical memory (bytes)=364318720\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6416130048\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837807\n",
      "22/01/01 20:50:52 INFO HadoopUtil: Deleting dsm010/vodacom-corpus-vectors/partial-vectors-0\n",
      "22/01/01 20:50:52 INFO SparseVectorsFromSequenceFiles: Calculating IDF\n",
      "22/01/01 20:50:52 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:50:52 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242083\n",
      "22/01/01 20:50:53 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 20:50:53 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 20:50:53 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242083\n",
      "22/01/01 20:50:53 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:50:53 INFO YarnClientImpl: Submitted application application_1626049283275_242083\n",
      "22/01/01 20:50:53 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242083/\n",
      "22/01/01 20:50:53 INFO Job: Running job: job_1626049283275_242083\n",
      "22/01/01 20:50:59 INFO Job: Job job_1626049283275_242083 running in uber mode : false\n",
      "22/01/01 20:50:59 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:51:06 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:51:11 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:51:11 INFO Job: Job job_1626049283275_242083 completed successfully\n",
      "22/01/01 20:51:11 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=148952\n",
      "\t\tFILE: Number of bytes written=828859\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837966\n",
      "\t\tHDFS: Number of bytes written=212913\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19225\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16515\n",
      "\t\tTotal time spent by all map tasks (ms)=3845\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3303\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3845\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3303\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19686400\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16911360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8215\n",
      "\t\tMap output records=560144\n",
      "\t\tMap output bytes=6721728\n",
      "\t\tMap output materialized bytes=148952\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=560144\n",
      "\t\tCombine output records=10639\n",
      "\t\tReduce input groups=10639\n",
      "\t\tReduce shuffle bytes=148952\n",
      "\t\tReduce input records=10639\n",
      "\t\tReduce output records=10639\n",
      "\t\tSpilled Records=21278\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=82\n",
      "\t\tCPU time spent (ms)=4900\n",
      "\t\tPhysical memory (bytes) snapshot=683683840\n",
      "\t\tVirtual memory (bytes) snapshot=12775059456\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=391483392\n",
      "\t\tPeak Map Virtual memory (bytes)=6389100544\n",
      "\t\tPeak Reduce Physical memory (bytes)=292200448\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6385958912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=212913\n",
      "22/01/01 20:51:11 INFO SparseVectorsFromSequenceFiles: Pruning\n",
      "22/01/01 20:51:11 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n",
      "22/01/01 20:51:11 INFO deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress\n",
      "22/01/01 20:51:11 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "22/01/01 20:51:11 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:51:11 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242084\n",
      "22/01/01 20:51:11 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 20:51:11 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 20:51:11 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242084\n",
      "22/01/01 20:51:11 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:51:12 INFO YarnClientImpl: Submitted application application_1626049283275_242084\n",
      "22/01/01 20:51:12 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242084/\n",
      "22/01/01 20:51:12 INFO Job: Running job: job_1626049283275_242084\n",
      "22/01/01 20:51:18 INFO Job: Job job_1626049283275_242084 running in uber mode : false\n",
      "22/01/01 20:51:18 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:51:24 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:51:29 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:51:30 INFO Job: Job job_1626049283275_242084 completed successfully\n",
      "22/01/01 20:51:30 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1647612\n",
      "\t\tFILE: Number of bytes written=3402579\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837966\n",
      "\t\tHDFS: Number of bytes written=5837807\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17810\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17765\n",
      "\t\tTotal time spent by all map tasks (ms)=3562\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3553\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3562\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3553\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18237440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18191360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8215\n",
      "\t\tMap output records=8215\n",
      "\t\tMap output bytes=5770877\n",
      "\t\tMap output materialized bytes=1434711\n",
      "\t\tInput split bytes=159\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8215\n",
      "\t\tReduce shuffle bytes=1434711\n",
      "\t\tReduce input records=8215\n",
      "\t\tReduce output records=8215\n",
      "\t\tSpilled Records=16430\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tCPU time spent (ms)=5840\n",
      "\t\tPhysical memory (bytes) snapshot=742236160\n",
      "\t\tVirtual memory (bytes) snapshot=12819247104\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=391000064\n",
      "\t\tPeak Map Virtual memory (bytes)=6401290240\n",
      "\t\tPeak Reduce Physical memory (bytes)=351236096\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6417956864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837807\n",
      "22/01/01 20:51:30 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:51:30 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242085\n",
      "22/01/01 20:51:30 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 20:51:30 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 20:51:30 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242085\n",
      "22/01/01 20:51:30 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:51:30 INFO YarnClientImpl: Submitted application application_1626049283275_242085\n",
      "22/01/01 20:51:30 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242085/\n",
      "22/01/01 20:51:30 INFO Job: Running job: job_1626049283275_242085\n",
      "22/01/01 20:51:36 INFO Job: Job job_1626049283275_242085 running in uber mode : false\n",
      "22/01/01 20:51:36 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:51:41 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:51:47 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:51:48 INFO Job: Job job_1626049283275_242085 completed successfully\n",
      "22/01/01 20:51:48 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5802947\n",
      "\t\tFILE: Number of bytes written=12136895\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837976\n",
      "\t\tHDFS: Number of bytes written=5837807\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16595\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16955\n",
      "\t\tTotal time spent by all map tasks (ms)=3319\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3391\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3319\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3391\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16993280\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17361920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8215\n",
      "\t\tMap output records=8215\n",
      "\t\tMap output bytes=5770877\n",
      "\t\tMap output materialized bytes=5802947\n",
      "\t\tInput split bytes=169\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8215\n",
      "\t\tReduce shuffle bytes=5802947\n",
      "\t\tReduce input records=8215\n",
      "\t\tReduce output records=8215\n",
      "\t\tSpilled Records=16430\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=51\n",
      "\t\tCPU time spent (ms)=5120\n",
      "\t\tPhysical memory (bytes) snapshot=797409280\n",
      "\t\tVirtual memory (bytes) snapshot=12790018048\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=429518848\n",
      "\t\tPeak Map Virtual memory (bytes)=6393393152\n",
      "\t\tPeak Reduce Physical memory (bytes)=367890432\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6400598016\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837807\n",
      "22/01/01 20:51:48 INFO HadoopUtil: Deleting dsm010/vodacom-corpus-vectors/tf-vectors-partial\n",
      "22/01/01 20:51:48 INFO HadoopUtil: Deleting dsm010/vodacom-corpus-vectors/tf-vectors-toprune\n",
      "22/01/01 20:51:48 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:51:49 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242087\n",
      "22/01/01 20:51:49 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 20:51:49 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 20:51:49 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242087\n",
      "22/01/01 20:51:49 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:51:49 INFO YarnClientImpl: Submitted application application_1626049283275_242087\n",
      "22/01/01 20:51:49 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242087/\n",
      "22/01/01 20:51:49 INFO Job: Running job: job_1626049283275_242087\n",
      "22/01/01 20:51:55 INFO Job: Job job_1626049283275_242087 running in uber mode : false\n",
      "22/01/01 20:51:55 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:52:01 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:52:07 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:52:07 INFO Job: Job job_1626049283275_242087 completed successfully\n",
      "22/01/01 20:52:07 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5802947\n",
      "\t\tFILE: Number of bytes written=12140171\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6050851\n",
      "\t\tHDFS: Number of bytes written=5837807\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16660\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17635\n",
      "\t\tTotal time spent by all map tasks (ms)=3332\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3527\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3332\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3527\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17059840\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18058240\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8215\n",
      "\t\tMap output records=8215\n",
      "\t\tMap output bytes=5770877\n",
      "\t\tMap output materialized bytes=5802947\n",
      "\t\tInput split bytes=151\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8215\n",
      "\t\tReduce shuffle bytes=5802947\n",
      "\t\tReduce input records=8215\n",
      "\t\tReduce output records=8215\n",
      "\t\tSpilled Records=16430\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=48\n",
      "\t\tCPU time spent (ms)=5660\n",
      "\t\tPhysical memory (bytes) snapshot=766922752\n",
      "\t\tVirtual memory (bytes) snapshot=12808347648\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=408657920\n",
      "\t\tPeak Map Virtual memory (bytes)=6401171456\n",
      "\t\tPeak Reduce Physical memory (bytes)=358264832\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6407176192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837807\n",
      "22/01/01 20:52:07 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at lena-master/128.86.245.64:8032\n",
      "22/01/01 20:52:07 INFO JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jfoul001/.staging/job_1626049283275_242089\n",
      "22/01/01 20:52:07 INFO FileInputFormat: Total input files to process : 1\n",
      "22/01/01 20:52:07 INFO JobSubmitter: number of splits:1\n",
      "22/01/01 20:52:07 INFO JobSubmitter: Submitting tokens for job: job_1626049283275_242089\n",
      "22/01/01 20:52:07 INFO JobSubmitter: Executing with tokens: []\n",
      "22/01/01 20:52:08 INFO YarnClientImpl: Submitted application application_1626049283275_242089\n",
      "22/01/01 20:52:08 INFO Job: The url to track the job: http://lena-master:8088/proxy/application_1626049283275_242089/\n",
      "22/01/01 20:52:08 INFO Job: Running job: job_1626049283275_242089\n",
      "22/01/01 20:52:14 INFO Job: Job job_1626049283275_242089 running in uber mode : false\n",
      "22/01/01 20:52:14 INFO Job:  map 0% reduce 0%\n",
      "22/01/01 20:52:19 INFO Job:  map 100% reduce 0%\n",
      "22/01/01 20:52:25 INFO Job:  map 100% reduce 100%\n",
      "22/01/01 20:52:25 INFO Job: Job job_1626049283275_242089 completed successfully\n",
      "22/01/01 20:52:25 INFO Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5802947\n",
      "\t\tFILE: Number of bytes written=12137755\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5837965\n",
      "\t\tHDFS: Number of bytes written=5837807\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16420\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17110\n",
      "\t\tTotal time spent by all map tasks (ms)=3284\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3422\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3284\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3422\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16814080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17520640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8215\n",
      "\t\tMap output records=8215\n",
      "\t\tMap output bytes=5770877\n",
      "\t\tMap output materialized bytes=5802947\n",
      "\t\tInput split bytes=158\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8215\n",
      "\t\tReduce shuffle bytes=5802947\n",
      "\t\tReduce input records=8215\n",
      "\t\tReduce output records=8215\n",
      "\t\tSpilled Records=16430\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=46\n",
      "\t\tCPU time spent (ms)=5150\n",
      "\t\tPhysical memory (bytes) snapshot=773906432\n",
      "\t\tVirtual memory (bytes) snapshot=12819382272\n",
      "\t\tTotal committed heap usage (bytes)=2105540608\n",
      "\t\tPeak Map Physical memory (bytes)=422268928\n",
      "\t\tPeak Map Virtual memory (bytes)=6408830976\n",
      "\t\tPeak Reduce Physical memory (bytes)=355397632\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6414524416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5837807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5837807\n",
      "22/01/01 20:52:25 INFO HadoopUtil: Deleting dsm010/vodacom-corpus-vectors/partial-vectors-0\n",
      "22/01/01 20:52:25 INFO MahoutDriver: Program took 161224 ms (Minutes: 2.6870666666666665)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mahout seq2sparse \\\n",
    "    -nv \\\n",
    "    -i dsm010/vodacom-corpus-seqfiles \\\n",
    "    -o dsm010/vodacom-corpus-vectors \\\n",
    "    -ow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Clustering\n",
    "\n",
    "Perform clustering and output data to perform hyperparameter optimization by trying various distance measures and values of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "# range to use for k\n",
    "k_start=2\n",
    "k_end=20\n",
    "\n",
    "# the path to the vectors and dictionary\n",
    "path_vectors=dsm010/vodacom-corpus-vectors/tf-vectors\n",
    "path_dictionary=dsm010/vodacom-corpus-vectors/dictionary.file-*\n",
    "\n",
    "# the output base path for the clusters and the result local output path\n",
    "path_hdfs_base=hdfs://lena/user/jfoul001/\n",
    "path_clusters_base=dsm010/vodacom-corpus-kmeans\n",
    "path_results_base=~/code/dsm010-2021-oct/coursework_01/data/output/vodacom-corpus-clusters\n",
    "\n",
    "# the distance metric to use\n",
    "distance_metrics=(\"org.apache.mahout.common.distance.CosineDistanceMeasure\" \"org.apache.mahout.common.distance.EuclideanDistanceMeasure\" \"org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure\" \"org.apache.mahout.common.distance.ManhattanDistanceMeasure\")\n",
    "\n",
    "for distance_metric in \"${distance_metrics[@]}\"\n",
    "do\n",
    "  echo \"--- Distance Metric: $distance_metric\"\n",
    "\n",
    "    # perform the canopy clustering\n",
    "    distance_name=${distance_metric##*.}\n",
    "    path_centroids=dsm010/vodacom-corpus-canopy-centroids/${distance_name}\n",
    "\n",
    "    mahout canopy \\\n",
    "      -i $path_vectors \\\n",
    "      -ow \\\n",
    "      -o $path_centroids \\\n",
    "      -dm $distance_metric \\\n",
    "      -t1 0.5 \\\n",
    "      -t2 0.3\n",
    "\n",
    "  for ((k = $k_start; k <= $k_end; k++))\n",
    "  do\n",
    "    # get k with a leading zero if required\n",
    "    k_padded=$(printf %02d $k)\n",
    "\n",
    "    # set the output path for the clusters\n",
    "    path_clusters=\"${path_clusters_base}/${distance_name}/${k_padded}\"\n",
    "\n",
    "    echo \"---- K: $k_padded -- $path_clusters\"\n",
    "\n",
    "    # perform the kmeans clustering\n",
    "    mahout kmeans \\\n",
    "    -i $path_vectors \\\n",
    "    -c $path_centroids \\\n",
    "    -o \"${path_hdfs_base}${path_clusters}\" \\\n",
    "    -ow \\\n",
    "    -dm $distance_metric \\\n",
    "    -cl -cd 0.1 -ow -x 20 \\\n",
    "    -k $k\n",
    "\n",
    "    # set the path for output\n",
    "    path_final_clusters=`hadoop fs -ls -d -C \"${path_clusters}/clusters-*-final\"`\n",
    "    path_clusterpoints=\"${path_clusters}/clusteredPoints\"\n",
    "    path_results=\"${path_results_base}/${distance_name}/${k_padded}.txt\"\n",
    "\n",
    "    # output the cluster results\n",
    "    mahout clusterdump -dt sequencefile \\\n",
    "       -d $path_dictionary \\\n",
    "       -i $path_final_clusters  \\\n",
    "       -o $path_results \\\n",
    "       -of TEXT \\\n",
    "       -b 100 \\\n",
    "       -p $path_clusterpoints \\\n",
    "       -dm $distance_metric \\\n",
    "       -n 20 --evaluate\n",
    "  done  \n",
    "done"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "429f3e8d45833d845e6e031dbf3e229703adf0bd2129019c99d8da7ba46dd29e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('dsm010': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
